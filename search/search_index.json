{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Project Fleming","text":"<p>Introducing Project Fleming: Originally devised for Shell, this model-agnostic, AI-powered Discovery Tool harnesses open source models from Hugging Face on Small CPU Clusters on Databricks. We invite contributors and users alike to join us in enabling Discovery for all organizations looking to power their Inner Source initiatives or use Artificial Intelligence for specialized Discovery tasks.</p> <p>Inner Source refers to the deployment of Open Source practices of code and information sharing within a single organization. By sharing, we reduce duplication, free up cognitive space for innovation, and increase the speed of delivery.</p> <p>However, simply setting repositories to \u201copen\u201d is not enough to ensure effective sharing. Traditional search tools often have a narrow focus and rely on specificity.</p> <p>The solution is a Discovery tool, allowing developers to find unexpected yet helpful solutions and information. Leveraging the power of Artificial Intelligence, this tool is designed to be resource-efficient, using clever programming techniques to serve models with Databricks on a CPU cluster instead of a GPU one.</p> <p>Join us in contributing to Project Fleming and help drive innovation and efficiency through enhanced discovery and sharing.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/01/13/introducing-project-fleming-enhancing-code-reuse-and-efficiency-with-ai-discovery/","title":"Introducing Project Fleming: Enhancing Code Reuse and Efficiency with AI Discovery","text":"<p>We are delighted to announce the release of an enhancement to Project Fleming, with the addition of the Code Summary functionality. Once code has been summarised, this can be passed into Project Fleming's corpus to improve search for specific code functionality. </p>"},{"location":"blog/2025/01/13/introducing-project-fleming-enhancing-code-reuse-and-efficiency-with-ai-discovery/#code-summary","title":"Code Summary","text":"<p>The purpose of the Code Summary package is to improve a user's search for explicit code functionality, by utilising LLMs to ingest source code of repositories and generate comprehensive and descriptive documentation, that can then be indexed by Project Fleming and consequently improving the quality of results.</p> <p>The initial use-case of Project Fleming was to ingest repository readme contents into the search index, thereby allowing users to perform natural language searches, based off the information provided in the readme file by the developers. However, this was inherently limited by the fact it was reliant on the developer producing a descriptive readme. Furthermore, in some cases developers are unable to write a full functional description of all their code within a larger repository, as they are (reasonably) trying to give a high level overview of the entire project, instead of detailing specific code functionality.</p> <p>Enter Project Fleming's Code Summary Package, which can be tailored to generate descriptive code functionality documentation for repositories, thus increasing the likelihood that specific code relevant to the users query is found when this documentation is ingested into the search index of Project Fleming. This use of LLMs is designed to allow users to find reusable code, that may have been previously hidden by the domain context it is situated in, as it is no longer reliant on the specific functionality being described in the readme.</p>"},{"location":"blog/2025/01/13/introducing-project-fleming-enhancing-code-reuse-and-efficiency-with-ai-discovery/#further-applications","title":"Further applications","text":"<p>We see this as having multiple applications, not only to enhance the findability of reusable code, but also to help organisations with large-scale code scanning to find specific types of projects. For example, the Code Summary package could be used to identify AI projects, which could then be assessed to see if they are compliant with the local regulations. This is only one of many potential applications, and as always, we actively welcome feedback and further contributions to further enhance Project Fleming.</p>"},{"location":"blog/2025/04/14/introducing-the-new-generic-ui-for-fleming/","title":"Introducing the New Generic UI for Fleming","text":"<p>We are excited to announce the launch of our new generic user interface (UI) for Fleming!</p>"},{"location":"blog/2025/04/14/introducing-the-new-generic-ui-for-fleming/#how-to-guide-generic-frontend-for-project-fleming","title":"How-To Guide: Generic Frontend for Project Fleming","text":"<p>(Guide current as of: Monday, April 14, 2025)</p>"},{"location":"blog/2025/04/14/introducing-the-new-generic-ui-for-fleming/#introduction","title":"Introduction","text":"<p>Welcome! This guide explains how to set up, run, and understand the Generic Frontend for Project Fleming.</p> <p>What is this project? This is an open-source frontend application built with Next.js. Its primary purpose is to serve as a Generic User Interface (UI) that demonstrates how to effectively interact with the APIs provided by Project Fleming. It provides a practical example and a starting point for developers looking to leverage Project Fleming's capabilities within their own applications or environments.</p> <p>What does Project Fleming do? Project Fleming is an AI-powered Discovery Tool, originally developed for Shell and now available as open source. Here's what it does:</p> <ul> <li>Enhances Discovery: It helps users find relevant information and resources within an organization, such as code repositories, documentation (like SEMs and DALPs), Stack Overflow articles, and more, even when they don't know exactly what to search for. This goes beyond traditional search tools which often require specific keywords.</li> <li>Supports Inner Source: It's particularly useful for organizations adopting \"Inner Source\" practices (applying open source principles internally). By making internal projects and information more discoverable, it helps reduce duplication, encourages code reuse, speeds up delivery, and fosters innovation.</li> <li>Leverages AI: It uses open-source Artificial Intelligence models (e.g., from Hugging Face, using NLP and LLMs) to understand the meaning and context behind user queries and the indexed content, allowing it to find unexpected yet helpful solutions.</li> <li>Resource-Efficient: Designed to be model-agnostic and efficient, it can run effectively using clever programming techniques on Small CPU Clusters (e.g., on Databricks), making powerful AI-driven discovery accessible without necessarily requiring expensive GPU hardware.</li> </ul> <p>Essentially, Project Fleming aims to solve the challenge of information discovery in complex enterprise environments, enabling developers and others to find valuable internal resources they might not have otherwise known existed. This frontend application showcases how to interact with the Project Fleming API to achieve this.</p> <p>This guide will walk you through: 1.  Prerequisites 2.  Setting up the development environment 3.  Configuring the application to connect to your Project Fleming ML API 4.  Running the application locally 5.  Understanding the basic architecture</p>"},{"location":"blog/2025/04/14/introducing-the-new-generic-ui-for-fleming/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed and available:</p> <ol> <li>Node.js: (Version 20.x or later recommended for Next.js). You can download it from nodejs.org.</li> <li>Package Manager: <code>npm</code> (comes with Node.js), <code>yarn</code>, <code>pnpm</code>, or <code>bun</code>. Choose one you are comfortable with.</li> <li>Git: For cloning the repository.</li> <li>Project Fleming ML API Endpoint: You need the URL where your instance of the Project Fleming ML backend API is running.</li> <li>API Authentication Token: You need the authentication token required to securely access the Project Fleming ML API.</li> </ol> <p>(You should obtain the ML API endpoint and Auth Token from the team or documentation related to your specific Project Fleming backend deployment.)</p>"},{"location":"blog/2025/04/14/introducing-the-new-generic-ui-for-fleming/#getting-started-setup-and-running","title":"Getting Started: Setup and Running","text":"<p>Follow these steps to get the frontend running on your local machine:</p> <ol> <li> <p>Clone the Repository:     Open your terminal or command prompt and run:     <pre><code>git clone &lt;repository-url&gt; # Replace &lt;repository-url&gt; with the actual URL of your frontend's Git repository\ncd &lt;repository-directory&gt;   # Navigate into the cloned project folder\n</code></pre></p> </li> <li> <p>Install Dependencies:     Using your preferred package manager, install the necessary project dependencies:     <pre><code># Using npm\nnpm install\n\n# Or using yarn\nyarn install\n\n# Or using pnpm\npnpm install\n\n# Or using bun\nbun install\n</code></pre></p> </li> <li> <p>Configure Environment Variables:     This application needs to know where the Project Fleming ML API is located and how to authenticate with it.</p> <ul> <li>Create a new file named <code>.env</code> in the root directory of the project.</li> <li>Open the <code>.env</code> file and add the following lines, replacing the placeholder values with your actual API details:</li> </ul> <p><pre><code># .env file content\n\n# The full URL to your Project Fleming ML API endpoint\nML_API=https://your-project-fleming-api.example.com/api/search\n\n# The authentication token required by the API\nAPI_AUTH_TOKEN=your_secret_auth_token_here\n</code></pre> * Important: Ensure the <code>ML_API</code> value is the correct endpoint provided by your Project Fleming backend for making search/prediction requests. Save the <code>.env</code> file. This file is typically ignored by Git (via <code>.gitignore</code>) to keep your secrets safe.</p> </li> <li> <p>Run the Development Server:     Start the Next.js development server:     <pre><code># Using npm\nnpm run dev\n\n# Or using yarn\nyarn dev\n\n# Or using pnpm\npnpm dev\n\n# Or using bun\nbun dev\n</code></pre></p> </li> <li> <p>Access the Application:     Open your web browser and navigate to:     <code>http://localhost:3000</code></p> <p>You should now see the generic frontend application running.</p> </li> </ol>"},{"location":"blog/2025/04/14/introducing-the-new-generic-ui-for-fleming/#how-it-works-architecture-overview","title":"How It Works: Architecture Overview","text":"<p>Understanding the flow of data is key to using and potentially customizing this frontend:</p> <ol> <li>User Interaction: The user interacts with the UI, likely entering a search query into an input field in the page located at <code>app/page.tsx</code>.</li> <li>Frontend API Call: When the user submits their query, the frontend code (in <code>app/page.tsx</code> or related components) makes a request to its own internal API route: <code>/api/predictions</code>.</li> <li>Internal API Route (<code>/api/predictions</code>):<ul> <li>This Next.js API route (defined in <code>app/api/predictions/route.ts</code>) acts as a proxy or intermediary.</li> <li>It receives the request from the frontend UI.</li> <li>It reads the <code>ML_API</code> and <code>API_AUTH_TOKEN</code> from the environment variables (which you set in the <code>.env</code> file).</li> <li>It then makes a secure request (using the auth token) to the actual Project Fleming ML API (the URL specified in <code>ML_API</code>).</li> <li>It receives the raw prediction data (e.g., list of repos, articles) from the ML API.</li> <li>It perform some basic processing or reformatting on this data to make it suitable for the frontend UI.</li> <li>It sends the processed data back as a response to the frontend UI component that initially called it.</li> </ul> </li> <li>Displaying Results: The frontend UI component (<code>app/page.tsx</code>) receives the processed data from <code>/api/predictions</code> and renders the results (e.g., a list of findings) for the user to see.</li> </ol> <p>This architecture keeps your ML API endpoint and token secure on the server-side (within the <code>/api/predictions</code> route) rather than exposing them directly in the browser.</p>"},{"location":"blog/2025/04/14/introducing-the-new-generic-ui-for-fleming/#using-the-frontend","title":"Using the Frontend","text":"<p>Once running, you can typically:</p> <ol> <li>Find the search input field on the page (<code>http://localhost:3000</code>).</li> <li>Type in your query related to the kind of information Project Fleming indexes (e.g., \"how to implement authentication in python\", \"database connection pooling best practices\", \"find repository for UI components\").</li> <li>Submit the query (e.g., by pressing Enter or clicking a search button).</li> <li>View the results displayed on the page, which are fetched from your configured Project Fleming ML API via the frontend's internal API.</li> </ol>"},{"location":"blog/2025/04/14/introducing-the-new-generic-ui-for-fleming/#customization","title":"Customization","text":"<p>If you want to modify the frontend:</p> <ul> <li>UI Changes: Edit the React component file located at <code>app/page.tsx</code>. This is where the main page structure, input fields, and results display logic reside. The page will auto-update in your browser as you save changes when the development server is running.</li> <li>Data Handling/Processing: If you need to change how data from the ML API is processed before being shown in the UI, modify the internal API route handler (<code>app/api/predictions/route.ts</code> or a similar file depending on your specific setup).</li> </ul>"},{"location":"blog/2025/04/14/introducing-the-new-generic-ui-for-fleming/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Error connecting to API / No results:<ul> <li>Double-check the <code>ML_API</code> URL in your <code>.env</code> file. Is it correct and accessible? Is the path correct?</li> <li>Verify the <code>API_AUTH_TOKEN</code> in your <code>.env</code> file. Is it valid?</li> <li>Ensure the Project Fleming backend server is running and reachable from where you are running the frontend development server.</li> <li>Check the console output in your terminal (where you ran <code>npm run dev</code>) and the browser's developer console (F12) for specific error messages. Errors might originate from the <code>/api/predictions</code> route or the Project Fleming backend itself.</li> </ul> </li> <li>Application doesn't start:<ul> <li>Ensure all dependencies were installed correctly (<code>npm install</code> or equivalent).</li> <li>Make sure you have a compatible version of Node.js installed.</li> <li>Check if the <code>.env</code> file exists, even if you haven't filled it yet, as some setups might expect it.</li> </ul> </li> </ul>"},{"location":"blog/2025/04/14/introducing-the-new-generic-ui-for-fleming/#further-learning","title":"Further Learning","text":"<ul> <li>Project Fleming: Refer to the main documentation for Project Fleming for details about its ML models, API capabilities, backend setup, and contribution guidelines.</li> <li>Next.js:<ul> <li>Next.js Documentation</li> <li>Learn Next.js Interactive Tutorial</li> </ul> </li> </ul>"},{"location":"blog/2024/11/19/introducing-project-fleming-enhancing-code-reuse-and-efficiency-with-ai-discovery/","title":"Introducing Project Fleming: Enhancing Code Reuse and Efficiency with AI Discovery","text":"<p>Today, Shell\u2019s Inner Source Enablement Team has released key packages for a new AI-powered Discovery Tool. These packages are available for Open Source use.</p> <p>The Discovery Tool was created to address a specific Inner Source challenge: while setting a code repository to \u201copen\u201d is a good start, integrating it into a workflow using natural language search takes it further. Discovery is not just about finding what you expect; it\u2019s also about uncovering what you may not have anticipated. By leveraging AI, we make this possible. The tool can search for code as well as Stack Overflow entries. Our goal is to ensure that no developer leaves empty-handed, ultimately enhancing efficiency. Already, the tool has helped developers identify opportunities for code reuse.</p> <p>The idea for this tool originated from PhD research at Loughborough University, which demonstrated how powerful AI could be in classifying information, even when it was in an unstructured state.</p> <p>This tool would not have come to life without the skill, dedication, and creativity of the Inner Source Enablement Team and all those who supported us. Innovation rarely happens in isolation, and this was no exception. The team not only brought the concept to life but made it efficient, for example, by running it on a CPU instead of a GPU setup. Furthermore, we have striven to make it model-agnostic, thus allowing users to integrate the LLM of their choice.</p> <p>Specific thanks go to the following individuals:</p> <p>Shell</p> <ul> <li>Benedict Butcher</li> <li>Amber Rigg</li> <li>Tugce Ozberk Yener</li> <li>Oliver Carr</li> <li>Gabriel Molina</li> <li>Tom Lewis</li> <li>Anusha Modwal</li> <li>Michelle Bhaskaran</li> <li>Herman Kruis</li> <li>Niranjan Girhe</li> <li>Neethi Mary Regi</li> <li>Namitha Raveendranathan.</li> </ul> <p>Loughborough University</p> <ul> <li>Professor Jenny Harding</li> <li>Dr Diana Segura-Velandia</li> </ul> <p>We also could not have achieved this without management support from Bryce Bartmann, Dan Jeavons, Warren Harding, Adam Jordan and Karina Fernandez.</p> <p>We encourage all colleagues and potential collaborators interested in Inner Source and Open Source to explore the tool. We welcome your contributions as we continue its development. We also look forward to unexpected insights and new features that others may add.</p> <p>As always, we value your feedback - feedback is a gift.</p>"},{"location":"blog/2025/05/02/a-new-era-of-enterprise-ai-project-fleming-now-integrated-with-mosaic-ai-by-databricks/","title":"A New Era of Enterprise AI: Project Fleming Now Integrated with Mosaic AI by Databricks","text":"<p>Project Fleming\u2014the open-source AI discovery framework can now be enahcnes with Mosaic AI by Databricks. Empowering teams with open-source flexibility and enterprise-grade reliability</p> <p>With this integration, users can now harness the exploratory power of Project Fleming\u2014known for its intuitive discovery tools, branching logic, and open-source flexibility\u2014directly within the Databricks Data Intelligence Platform.</p>"},{"location":"blog/2025/05/02/a-new-era-of-enterprise-ai-project-fleming-now-integrated-with-mosaic-ai-by-databricks/#key-features-of-mosaic-ai-available-in-project-fleming","title":"\ud83d\udd0d Key Features of Mosaic AI available in Project Fleming","text":""},{"location":"blog/2025/05/02/a-new-era-of-enterprise-ai-project-fleming-now-integrated-with-mosaic-ai-by-databricks/#permission-and-rate-limiting","title":"\ud83d\udd10 Permission and Rate Limiting","text":"<p>Purpose: Controls who can access model endpoints and how much they can use them.</p> <p>Functionality: Allows administrators to define access policies and rate limits per user or group, ensuring fair usage and preventing abuse.</p> <p>Support: Available for most endpoint types, including external models, foundation model APIs (both provisioned and pay-per-token), and custom models. Not supported for Mosaic AI agents.</p>"},{"location":"blog/2025/05/02/a-new-era-of-enterprise-ai-project-fleming-now-integrated-with-mosaic-ai-by-databricks/#payload-logging","title":"\ud83d\udce6 Payload Logging","text":"<p>Purpose: Enables monitoring and auditing of data sent to model APIs by logging requests and responses into inference tables.</p> <p>Functionality: Captures and stores input/output payloads for each model call, enabling traceability, debugging, and compliance auditing.</p> <p>Support: Fully supported across all endpoint types, including Mosaic AI agents.</p>"},{"location":"blog/2025/05/02/a-new-era-of-enterprise-ai-project-fleming-now-integrated-with-mosaic-ai-by-databricks/#usage-tracking","title":"\ud83d\udcca Usage Tracking","text":"<p>Purpose: Tracks operational usage and costs of model endpoints using system tables.</p> <p>Functionality: Aggregates and reports on endpoint activity, including call frequency, latency, and cost metrics, to support budgeting and optimization.</p> <p>Support: Supported for all endpoint types except Mosaic AI agents.</p>"},{"location":"blog/2025/05/02/a-new-era-of-enterprise-ai-project-fleming-now-integrated-with-mosaic-ai-by-databricks/#traffic-splitting","title":"\u2696\ufe0f Traffic Splitting","text":"<p>Purpose: Load balances traffic across multiple models.</p> <p>Functionality: Useful for A/B testing or gradual rollouts.</p> <p>Support: Supported for external, provisioned foundation model APIs, and custom model endpoints.</p>"},{"location":"blog/2025/05/02/a-new-era-of-enterprise-ai-project-fleming-now-integrated-with-mosaic-ai-by-databricks/#further-features-of-mosaic-ai-available-with-databricks","title":"\ud83d\udd0d Further Features of Mosaic AI available with Databricks","text":""},{"location":"blog/2025/05/02/a-new-era-of-enterprise-ai-project-fleming-now-integrated-with-mosaic-ai-by-databricks/#ai-guardrails","title":"\ud83d\udd10 AI Guardrails","text":"<p>Purpose: Prevents unsafe or non-compliant data from being processed.</p> <p>Functionality: Filters out harmful or unwanted content in both requests and responses.</p> <p>Support: Available for most model endpoints except Mosaic AI agents.</p>"},{"location":"blog/2025/05/02/a-new-era-of-enterprise-ai-project-fleming-now-integrated-with-mosaic-ai-by-databricks/#fallbacks","title":"\ud83d\udd01 Fallbacks","text":"<p>Purpose: Ensures reliability by minimizing production outages.</p> <p>Functionality: Automatically switches to backup models or endpoints if the primary fails.</p> <p>Support: Currently only supported for external model endpoints.</p>"},{"location":"blog/2025/05/02/a-new-era-of-enterprise-ai-project-fleming-now-integrated-with-mosaic-ai-by-databricks/#why-this-matters","title":"\ud83c\udf10 Why This Matters","text":"<p>This integration isn\u2019t just a technical upgrade\u2014it\u2019s a strategic enabler. Enterprises can now: - Build compound AI systems faster - Ensure governance and compliance from day one - Empower teams with open-source flexibility and enterprise-grade reliability</p>"},{"location":"blog/2025/05/02/a-new-era-of-enterprise-ai-project-fleming-now-integrated-with-mosaic-ai-by-databricks/#get-involved-contribute-to-project-fleming","title":"\ud83e\udd1d Get Involved: Contribute to Project Fleming","text":"<p>Project Fleming thrives on the power of community. Whether you're a developer, researcher, data scientist, or AI enthusiast, your contributions can help shape the future of open-source AI discovery.</p> <p>Here\u2019s how you can get involved: - \u2b50 Star the project on GitHub - \ud83e\udde0 Join discussions and share your ideas in the community forums - \ud83d\udce2 Spread the word by writing blog posts, tutorials, or hosting meetups</p> <p>Together, we can build a smarter, more open AI future.</p>"},{"location":"blog/2025/05/29/unlocking-multi-endpoint-search-in-project-flemings-generic-frontend/","title":"Unlocking Multi-Endpoint Search in Project Fleming's Generic Frontend","text":"<p>Exciting new capabilities have arrived in Project Fleming\u2019s generic frontend, transforming the way users interact with multiple Databricks endpoints. With the latest update, users can now add multiple endpoints through the tools UI, enabling seamless searches across all configured sources simultaneously. \ud83d\udd0d  </p>"},{"location":"blog/2025/05/29/unlocking-multi-endpoint-search-in-project-flemings-generic-frontend/#whats-new","title":"\ud83c\udfaf What\u2019s New?","text":"<ul> <li>Multi-endpoint integration \u2013 Users can now connect multiple Databricks endpoints within the UI, bringing varied datasets into one cohesive search experience. </li> <li>Unified search \u2013 Instead of querying individual endpoints separately, users can now perform searches across all added sources in one go. \u26a1  </li> <li>Flexible results navigation \u2013 Results can be explored efficiently, with users able to switch between different tabs to compare data from each endpoint. </li> </ul>"},{"location":"blog/2025/05/29/unlocking-multi-endpoint-search-in-project-flemings-generic-frontend/#why-this-matters","title":"\ud83d\udd25 Why This Matters","text":"<p>This enhancement is a game-changer for workflows that require cross-endpoint searches. Whether performing bespoke searches across multiple datasets or comparing model results during testing, users can now streamline their work within a single tool. This update eliminates the need for manual aggregation and significantly improves efficiency in data analysis. \ud83d\ude80  </p> <p>By integrating multi-endpoint functionality into the generic frontend, Project Fleming is empowering users with a more connected, more efficient, and more adaptable search environment.  </p>"},{"location":"blog/2025/05/29/unlocking-multi-endpoint-search-in-project-flemings-generic-frontend/#get-involved-contribute-to-project-fleming","title":"\ud83e\udd1d Get Involved: Contribute to Project Fleming","text":"<p>Project Fleming thrives on the power of community. Whether you're a developer, researcher, data scientist, or AI enthusiast, your contributions can help shape the future of open-source AI discovery.</p> <p>Here\u2019s how you can get involved: - \u2b50 Star the project on GitHub - \ud83e\udde0 Join discussions and share your ideas in the community forums - \ud83d\udce2 Spread the word by writing blog posts, tutorials, or hosting meetups</p> <p>Together, we can build a smarter, more open AI future.</p>"},{"location":"blog/2025/02/28/project-fleming-is-now-available-on-pypi/","title":"Project Fleming is now available on PyPI","text":"<p>Exciting News!</p> <p>We are thrilled to announce that Project Fleming is now available on PyPI! \ud83c\udf89</p> <p>Project Fleming is designed to make your development process smoother and more efficient. To get started, simply install it using pip:</p> <pre><code>pip install project-fleming\n</code></pre> <p>For more details and to try it out, visit our PyPI page: Project Fleming on PyPI. Stay tuned for further updates and examples of Project Fleming!</p> <p>We can't wait for you to explore and benefit from Project Fleming. Happy coding!</p>"},{"location":"code-reference/4o-MiniSummary/","title":"Code Summary with GPT-4oMini","text":""},{"location":"code-reference/4o-MiniSummary/#src.fleming.code_summary.fourO_mini_summary.OpenAIClient","title":"<code>OpenAIClient</code>","text":"<p>Class to interact with the OpenAI API to generate documentation using AI based on a prompt and source code.</p> <p>The class contains the following methods:</p> <ol> <li>call_openai: Authenticate to and call the OpenAI API to generate documentation based on the provided prompt and source code.</li> <li>save_results: Save the generated results to a specified output table.</li> </ol>"},{"location":"code-reference/4o-MiniSummary/#src.fleming.code_summary.fourO_mini_summary.OpenAIClient--example","title":"Example","text":"<pre><code>from fleming.code_summary.fourO_mini_summary import call_openai\nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"openai_client\").getOrCreate()\n\nspark_input_df = \"your_spark_input_df\"\noutput_table_name = \"your_output_table\"\n\nprompt = \"The following code is the contents of a repository, generate a short summary paragraph describing what the repository purpose is. A paragraph detailing the key functionalities and technologies integrate with and a list of key words associated with this repository underneath. Focus on the purpose of the code contained in the repository, and the technologies, data and platforms it integrates with\"\n\napi_key = \"your_api_key\"\nendpoint = \"https://api.openai.com/yourendpointhere\"\n\nheaders = {\n\"Content-Type\": \"application/json\",\n\"api-key\": api_key,\n}\n\nclient = OpenAIClient(spark, delta_table, output_table_name, prompt, api_key, endpoint, headers)\nclient.call_openai()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>input_spark_df</code> <code>DataFrame</code> <p>Source spark DataFrame containing the input data</p> required <code>output_table_name</code> <code>str</code> <p>Name of the output table to save results</p> required <code>prompt</code> <code>str</code> <p>Prompt to send to the OpenAI API</p> required <code>api_key</code> <code>str</code> <p>API key for OpenAI</p> required <code>endpoint</code> <code>str</code> <p>Endpoint for OpenAI API</p> required <code>headers</code> <code>dict</code> <p>Headers for the API request</p> required Source code in <code>src/fleming/code_summary/fourO_mini_summary.py</code> <pre><code>class OpenAIClient:\n    \"\"\"\n    Class to interact with the OpenAI API to generate documentation using AI based on a prompt and source code.\n\n    The class contains the following methods:\n\n    1. call_openai: Authenticate to and call the OpenAI API to generate documentation based on the provided prompt and source code.\n    2. save_results: Save the generated results to a specified output table.\n\n    Example\n    --------\n    ```python\n    from fleming.code_summary.fourO_mini_summary import call_openai\n    from pyspark.sql import SparkSession\n\n    # Not required if using Databricks\n    spark = SparkSession.builder.appName(\"openai_client\").getOrCreate()\n\n    spark_input_df = \"your_spark_input_df\"\n    output_table_name = \"your_output_table\"\n\n    prompt = \"The following code is the contents of a repository, generate a short summary paragraph describing what the repository purpose is. A paragraph detailing the key functionalities and technologies integrate with and a list of key words associated with this repository underneath. Focus on the purpose of the code contained in the repository, and the technologies, data and platforms it integrates with\"\n\n    api_key = \"your_api_key\"\n    endpoint = \"https://api.openai.com/yourendpointhere\"\n\n    headers = {\n    \"Content-Type\": \"application/json\",\n    \"api-key\": api_key,\n    }\n\n    client = OpenAIClient(spark, delta_table, output_table_name, prompt, api_key, endpoint, headers)\n    client.call_openai()\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        input_spark_df (DataFrame): Source spark DataFrame containing the input data\n        output_table_name (str): Name of the output table to save results\n        prompt (str): Prompt to send to the OpenAI API\n        api_key (str): API key for OpenAI\n        endpoint (str): Endpoint for OpenAI API\n        headers (dict): Headers for the API request\n    \"\"\"\n\n    spark: SparkSession\n    input_spark_df: DataFrame\n    output_table_name: str\n    prompt: str\n    api_key: str\n    endpoint: str\n    headers: dict\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        input_spark_df: DataFrame,\n        output_table_name: str,\n        prompt: str,\n        api_key: str,\n        endpoint: str,\n    ) -&gt; None:\n        self.spark = spark\n        self.input_spark_df = input_spark_df\n        self.output_table_name = output_table_name\n        self.prompt = prompt\n        self.api_key = api_key\n        self.endpoint = endpoint\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n            \"api-key\": self.api_key,\n        }\n        self.results_df = None  # Initialize an instance variable to store results\n\n    def call_openai(\n        self, title: str, concatenated_content: str, total_token_count: str\n    ) -&gt; None:\n        \"\"\"\n        Call the OpenAI API to generate summarised content based on the provided prompt and source content.\n\n        Parameters:\n            title (str): Column name for column containing summarised text title\n            concatenated_content (str): Column name for column containing concatenated content\n            total_token_count (str): Column name for column containing total token count\n\n        Returns:\n            results_df (DataFrame): PySpark DataFrame containing summarisation of each entry\n        \"\"\"\n\n        results = []\n        repo_contents_df = self.input_spark_df\n        repo_contents_df = repo_contents_df.limit(3)\n\n        for row in repo_contents_df.collect():\n            input_source_code = row[concatenated_content]\n            repo_name = row[title]\n            total_token_count = row[total_token_count]\n\n            payload = {\n                \"messages\": [\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\n                                \"type\": \"text\",\n                                \"text\": f\"{self.prompt}{input_source_code}\",\n                            }\n                        ],\n                    }\n                ],\n                \"temperature\": 0.0,\n                \"top_p\": 0.95,\n            }\n\n            try:\n                response = requests.post(\n                    self.endpoint, headers=self.headers, json=payload\n                )\n                response.raise_for_status()\n                json_dict = response.json()\n                output = json_dict[\"choices\"][0][\"message\"][\"content\"]\n                results.append((repo_name, self.prompt, total_token_count, output))\n                print(output)\n\n            except requests.RequestException as e:\n                logging.exception(\"Failed to make the request.\")\n                raise SystemExit(f\"Failed to make the request. Error: {e}\")\n\n        self.results_df = self.spark.createDataFrame(\n            results, [\"repo_name\", \"prompt\", \"repo_token_count\", \"virtual_readme\"]\n        )\n\n    def display_results(self) -&gt; None:\n        \"\"\"\n        Display the generated results.\n\n        Returns:\n            results_df(pyspark_df): returns image of dataframe\n        \"\"\"\n        if self.results_df is not None:\n            display(self.results_df)\n        else:\n            raise ValueError(\"No results to display. Please call call_openai() first.\")\n\n    def save_results(self) -&gt; None:\n        \"\"\"\n        Save the generated results to the specified output table.\n\n        Returns:\n            None\n        \"\"\"\n        if self.results_df is not None:\n            self.results_df.write.mode(\"overwrite\").option(\n                \"mergeSchema\", \"true\"\n            ).saveAsTable(f\"hive_metastore.dev_innersource.{self.output_table_name}\")\n        else:\n            raise ValueError(\"No results to save. Please call call_openai() first.\")\n</code></pre>"},{"location":"code-reference/4o-MiniSummary/#src.fleming.code_summary.fourO_mini_summary.OpenAIClient.call_openai","title":"<code>call_openai(title, concatenated_content, total_token_count)</code>","text":"<p>Call the OpenAI API to generate summarised content based on the provided prompt and source content.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Column name for column containing summarised text title</p> required <code>concatenated_content</code> <code>str</code> <p>Column name for column containing concatenated content</p> required <code>total_token_count</code> <code>str</code> <p>Column name for column containing total token count</p> required <p>Returns:</p> Name Type Description <code>results_df</code> <code>DataFrame</code> <p>PySpark DataFrame containing summarisation of each entry</p> Source code in <code>src/fleming/code_summary/fourO_mini_summary.py</code> <pre><code>def call_openai(\n    self, title: str, concatenated_content: str, total_token_count: str\n) -&gt; None:\n    \"\"\"\n    Call the OpenAI API to generate summarised content based on the provided prompt and source content.\n\n    Parameters:\n        title (str): Column name for column containing summarised text title\n        concatenated_content (str): Column name for column containing concatenated content\n        total_token_count (str): Column name for column containing total token count\n\n    Returns:\n        results_df (DataFrame): PySpark DataFrame containing summarisation of each entry\n    \"\"\"\n\n    results = []\n    repo_contents_df = self.input_spark_df\n    repo_contents_df = repo_contents_df.limit(3)\n\n    for row in repo_contents_df.collect():\n        input_source_code = row[concatenated_content]\n        repo_name = row[title]\n        total_token_count = row[total_token_count]\n\n        payload = {\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": f\"{self.prompt}{input_source_code}\",\n                        }\n                    ],\n                }\n            ],\n            \"temperature\": 0.0,\n            \"top_p\": 0.95,\n        }\n\n        try:\n            response = requests.post(\n                self.endpoint, headers=self.headers, json=payload\n            )\n            response.raise_for_status()\n            json_dict = response.json()\n            output = json_dict[\"choices\"][0][\"message\"][\"content\"]\n            results.append((repo_name, self.prompt, total_token_count, output))\n            print(output)\n\n        except requests.RequestException as e:\n            logging.exception(\"Failed to make the request.\")\n            raise SystemExit(f\"Failed to make the request. Error: {e}\")\n\n    self.results_df = self.spark.createDataFrame(\n        results, [\"repo_name\", \"prompt\", \"repo_token_count\", \"virtual_readme\"]\n    )\n</code></pre>"},{"location":"code-reference/4o-MiniSummary/#src.fleming.code_summary.fourO_mini_summary.OpenAIClient.display_results","title":"<code>display_results()</code>","text":"<p>Display the generated results.</p> <p>Returns:</p> Name Type Description <code>results_df</code> <code>pyspark_df</code> <p>returns image of dataframe</p> Source code in <code>src/fleming/code_summary/fourO_mini_summary.py</code> <pre><code>def display_results(self) -&gt; None:\n    \"\"\"\n    Display the generated results.\n\n    Returns:\n        results_df(pyspark_df): returns image of dataframe\n    \"\"\"\n    if self.results_df is not None:\n        display(self.results_df)\n    else:\n        raise ValueError(\"No results to display. Please call call_openai() first.\")\n</code></pre>"},{"location":"code-reference/4o-MiniSummary/#src.fleming.code_summary.fourO_mini_summary.OpenAIClient.save_results","title":"<code>save_results()</code>","text":"<p>Save the generated results to the specified output table.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fleming/code_summary/fourO_mini_summary.py</code> <pre><code>def save_results(self) -&gt; None:\n    \"\"\"\n    Save the generated results to the specified output table.\n\n    Returns:\n        None\n    \"\"\"\n    if self.results_df is not None:\n        self.results_df.write.mode(\"overwrite\").option(\n            \"mergeSchema\", \"true\"\n        ).saveAsTable(f\"hive_metastore.dev_innersource.{self.output_table_name}\")\n    else:\n        raise ValueError(\"No results to save. Please call call_openai() first.\")\n</code></pre>"},{"location":"code-reference/CorpusTextCreation/","title":"CorpusTextCreation","text":""},{"location":"code-reference/CorpusTextCreation/#src.fleming.discovery.corpus_creation.CorpusTextCreation","title":"<code>CorpusTextCreation</code>","text":"<p>Class to create the corpus txt file for the semantic search model from a dataframe.</p> <p>The class contains the following methods:</p> <ol> <li>concat_columns: Concatenate the columns to create the corpus from the dataframe. This will take all the columns in the dataframe and concatenate them to create the corpus in the correct format for the Fleming Frontend.</li> <li>write_corpus_to_file: Write the corpus to a file from the concatenated columns.</li> </ol>"},{"location":"code-reference/CorpusTextCreation/#src.fleming.discovery.corpus_creation.CorpusTextCreation--example","title":"Example","text":"<pre><code>from fleming.discovery.corpus_creation import CorpusCreation\nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"corpus_creation\").getOrCreate()\n\ncorpus_df = spark.read.csv(\"/tmp/corpus.csv\", header=True, inferSchema=True)\ncorpus_file_path = \"/tmp/search_corpus.txt\"\n\ncorpus_creation = CorpusCreation(corpus_df, corpus_file_path)\ncorpus = corpus_creation.concat_columns(\"RepoName\", \"RepoLink\", \"RepoDescription\")\ncorpus_creation.write_corpus_to_file(corpus)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>corpus_df</code> <code>df</code> <p>Source dataframe of the corpus</p> required <code>corpus_file_path</code> <code>str</code> <p>File path to write the corpus</p> required Source code in <code>src/fleming/discovery/corpus_creation.py</code> <pre><code>class CorpusTextCreation:\n    \"\"\"\n    Class to create the corpus txt file for the semantic search model from a dataframe.\n\n    The class contains the following methods:\n\n    1. concat_columns: Concatenate the columns to create the corpus from the dataframe. This will take all the columns in the dataframe and concatenate them to create the corpus in the correct format for the Fleming Frontend.\n    2. write_corpus_to_file: Write the corpus to a file from the concatenated columns.\n\n      Example\n    --------\n    ```python\n\n    from fleming.discovery.corpus_creation import CorpusCreation\n    from pyspark.sql import SparkSession\n\n    # Not required if using Databricks\n    spark = SparkSession.builder.appName(\"corpus_creation\").getOrCreate()\n\n    corpus_df = spark.read.csv(\"/tmp/corpus.csv\", header=True, inferSchema=True)\n    corpus_file_path = \"/tmp/search_corpus.txt\"\n\n    corpus_creation = CorpusCreation(corpus_df, corpus_file_path)\n    corpus = corpus_creation.concat_columns(\"RepoName\", \"RepoLink\", \"RepoDescription\")\n    corpus_creation.write_corpus_to_file(corpus)\n\n    ```\n\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        corpus_df (df): Source dataframe of the corpus\n        corpus_file_path (str): File path to write the corpus\n\n    \"\"\"\n\n    spark: SparkSession\n    corpus_df: DataFrame\n    corpus_file_path: str\n\n    def __init__(\n        self, spark: SparkSession, corpus_df: DataFrame, corpus_file_path: str\n    ) -&gt; None:\n        self.spark = spark\n        self.corpus_df = corpus_df\n        self.corpus_file_path = corpus_file_path\n\n    def concat_columns(self) -&gt; list:\n        \"\"\"\n        Concatenate the columns to create the corpus\n\n        Parameters:\n        None\n\n        Returns:\n        corpus(list): List of concatenated columns\n\n        \"\"\"\n        df = self.corpus_df.withColumn(\n            \"ConcatColumns\", concat_ws(\" \", *self.corpus_df.columns)\n        )\n\n        corpus = [row[\"ConcatColumns\"] for row in df.collect()]\n\n        return corpus\n\n    def concat_columns(\n        self, df, item_name_column, item_link_column, item_summmary_column\n    ) -&gt; list:\n        \"\"\"\n        Concatenate the columns to create the corpus\n\n        Parameters:\n        df(df): Cleaned dataframe\n\n        Returns:\n        corpus(list): List of concatenated columns with the format of each string consising of the following:\n\n        Example:\n        {\"Name\":\"Fleming\",\"Link\":\"https://github.com/sede-open/Fleming\",\"Summary\":\"Open-source project of the \\\"brain\\\" of the ai discovery tool. Includes technical scripts to build, register, serve and query models on databricks. Models can be run on cpu and not gpu providing signiifcant cost reductions. Databricks is utilized to build and train machine learning models on the ingested data. \"}{\"filter\":{\"LicenceFileContent\":\"Apache License 2.0\",\"Archived\":\"Active\"}}\n\n        \"\"\"\n\n        exclude_cols = [item_name_column, item_link_column, item_summmary_column]\n        include_cols = [c for c in df.columns if c not in exclude_cols]\n        map_expr = create_map(\n            *[item for c in include_cols for item in (lit(c), col(c))]\n        )\n        df = df.withColumn(\"filter\", map_expr)\n\n        df = df.withColumn(\n            \"dict_column\",\n            expr(\n                f\"map('Name', {item_name_column}, 'Link', {item_link_column}, 'Summary', {item_summmary_column})\"\n            ),\n        )\n\n        df = df.withColumn(\"filter\", create_map(lit(\"filter\"), col(\"filter\")))\n\n        df = df.withColumn(\"filter\", to_json(col(\"filter\")))\n        df = df.withColumn(\"dict_column\", to_json(col(\"dict_column\")))\n\n        df = df.withColumn(\n            \"ReadMe_W_Answer\", concat_ws(\"\", col(\"dict_column\"), col(\"filter\"))\n        )\n        df = df.withColumn(\n            \"ReadMe_W_Answer\", regexp_replace(\"ReadMe_W_Answer\", r\"\\}\\{\", \",\")\n        )\n\n        # COLLECTING ONLY THE README_W_ANSWER TEXT INFORMATION AS THE CORPUS\n        corpus = [row[\"ReadMe_W_Answer\"] for row in df.collect()]\n\n        return corpus\n\n    def write_corpus_to_file(self, corpus) -&gt; None:\n        \"\"\"\n        Write the corpus to a file\n\n        Parameters:\n        corpus(list): List of concatenated columns\n\n        Returns:\n        None\n        \"\"\"\n        with open(self.corpus_file_path, \"w\") as file:\n            for sentence in corpus:\n                try:\n                    file.write(sentence + \"\\n\")\n                    print(sentence)\n                except Exception as e:\n                    logging.exception(str(e))\n                    raise e\n</code></pre>"},{"location":"code-reference/CorpusTextCreation/#src.fleming.discovery.corpus_creation.CorpusTextCreation.concat_columns","title":"<code>concat_columns(df, item_name_column, item_link_column, item_summmary_column)</code>","text":"<p>Concatenate the columns to create the corpus</p> <p>Parameters: df(df): Cleaned dataframe</p> <p>Returns: corpus(list): List of concatenated columns with the format of each string consising of the following:</p> <p>Example: {\"Name\":\"Fleming\",\"Link\":\"https://github.com/sede-open/Fleming\",\"Summary\":\"Open-source project of the \"brain\" of the ai discovery tool. Includes technical scripts to build, register, serve and query models on databricks. Models can be run on cpu and not gpu providing signiifcant cost reductions. Databricks is utilized to build and train machine learning models on the ingested data. \"}{\"filter\":{\"LicenceFileContent\":\"Apache License 2.0\",\"Archived\":\"Active\"}}</p> Source code in <code>src/fleming/discovery/corpus_creation.py</code> <pre><code>def concat_columns(\n    self, df, item_name_column, item_link_column, item_summmary_column\n) -&gt; list:\n    \"\"\"\n    Concatenate the columns to create the corpus\n\n    Parameters:\n    df(df): Cleaned dataframe\n\n    Returns:\n    corpus(list): List of concatenated columns with the format of each string consising of the following:\n\n    Example:\n    {\"Name\":\"Fleming\",\"Link\":\"https://github.com/sede-open/Fleming\",\"Summary\":\"Open-source project of the \\\"brain\\\" of the ai discovery tool. Includes technical scripts to build, register, serve and query models on databricks. Models can be run on cpu and not gpu providing signiifcant cost reductions. Databricks is utilized to build and train machine learning models on the ingested data. \"}{\"filter\":{\"LicenceFileContent\":\"Apache License 2.0\",\"Archived\":\"Active\"}}\n\n    \"\"\"\n\n    exclude_cols = [item_name_column, item_link_column, item_summmary_column]\n    include_cols = [c for c in df.columns if c not in exclude_cols]\n    map_expr = create_map(\n        *[item for c in include_cols for item in (lit(c), col(c))]\n    )\n    df = df.withColumn(\"filter\", map_expr)\n\n    df = df.withColumn(\n        \"dict_column\",\n        expr(\n            f\"map('Name', {item_name_column}, 'Link', {item_link_column}, 'Summary', {item_summmary_column})\"\n        ),\n    )\n\n    df = df.withColumn(\"filter\", create_map(lit(\"filter\"), col(\"filter\")))\n\n    df = df.withColumn(\"filter\", to_json(col(\"filter\")))\n    df = df.withColumn(\"dict_column\", to_json(col(\"dict_column\")))\n\n    df = df.withColumn(\n        \"ReadMe_W_Answer\", concat_ws(\"\", col(\"dict_column\"), col(\"filter\"))\n    )\n    df = df.withColumn(\n        \"ReadMe_W_Answer\", regexp_replace(\"ReadMe_W_Answer\", r\"\\}\\{\", \",\")\n    )\n\n    # COLLECTING ONLY THE README_W_ANSWER TEXT INFORMATION AS THE CORPUS\n    corpus = [row[\"ReadMe_W_Answer\"] for row in df.collect()]\n\n    return corpus\n</code></pre>"},{"location":"code-reference/CorpusTextCreation/#src.fleming.discovery.corpus_creation.CorpusTextCreation.write_corpus_to_file","title":"<code>write_corpus_to_file(corpus)</code>","text":"<p>Write the corpus to a file</p> <p>Parameters: corpus(list): List of concatenated columns</p> <p>Returns: None</p> Source code in <code>src/fleming/discovery/corpus_creation.py</code> <pre><code>def write_corpus_to_file(self, corpus) -&gt; None:\n    \"\"\"\n    Write the corpus to a file\n\n    Parameters:\n    corpus(list): List of concatenated columns\n\n    Returns:\n    None\n    \"\"\"\n    with open(self.corpus_file_path, \"w\") as file:\n        for sentence in corpus:\n            try:\n                file.write(sentence + \"\\n\")\n                print(sentence)\n            except Exception as e:\n                logging.exception(str(e))\n                raise e\n</code></pre>"},{"location":"code-reference/ModelQuery/","title":"ModelQuery","text":""},{"location":"code-reference/ModelQuery/#src.fleming.discovery.model_query.ModelQuery","title":"<code>ModelQuery</code>","text":"<p>A class which allows for querying a model serving endpoint on databricks.</p> <p>This class is used to query a model serving endpoint on databricks with a dataset.</p>"},{"location":"code-reference/ModelQuery/#src.fleming.discovery.model_query.ModelQuery--example","title":"Example:","text":"<pre><code>url = \"https://example.com/model_endpoint\"\ntoken = \"your_auth_token\"\n\n# Create an instance of ModelQuery\nmodel_query = ModelQuery(url, token)\n\n# Example dataset\ndataset = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6]})\n\ntry:\n    # Score the model using the dataset\n    response = model_query.score_model(dataset)\n    print(response)\nexcept requests.exceptions.HTTPError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the model serving endpoint.</p> required <code>token</code> <code>str</code> <p>The authorization token for the model serving endpoint.</p> required Source code in <code>src/fleming/discovery/model_query.py</code> <pre><code>class ModelQuery:\n    \"\"\"\n    A class which allows for querying a model serving endpoint on databricks.\n\n    This class is used to query a model serving endpoint on databricks with a dataset.\n\n    Example:\n    --------\n    ```python\n\n    url = \"https://example.com/model_endpoint\"\n    token = \"your_auth_token\"\n\n    # Create an instance of ModelQuery\n    model_query = ModelQuery(url, token)\n\n    # Example dataset\n    dataset = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6]})\n\n    try:\n        # Score the model using the dataset\n        response = model_query.score_model(dataset)\n        print(response)\n    except requests.exceptions.HTTPError as e:\n        print(f\"Error: {str(e)}\")\n\n    ```\n\n    Parameters:\n        url (str): The URL of the model serving endpoint.\n        token (str): The authorization token for the model serving endpoint.\n    \"\"\"\n\n    url: str\n    token: str\n\n    def __init__(self, url, token):\n        self.url = url\n        self.token = token\n\n    def create_tf_serving_json(self, data):\n        \"\"\"\n        Creates a JSON object for TensorFlow serving.\n\n        Parameters:\n            data (Union[dict, pd.DataFrame, np.ndarray]): The input data.\n\n        Returns:\n            dict: The JSON object for TensorFlow serving.\n        \"\"\"\n        return {\n            \"inputs\": (\n                {name: data[name].tolist() for name in data.keys()}\n                if isinstance(data, dict)\n                else data.tolist()\n            )\n        }\n\n    def score_model(self, dataset):\n        \"\"\"\n        Scores the model using the provided dataset.\n\n        Parameters:\n            dataset (Union[pd.DataFrame, np.ndarray]): The dataset to be scored.\n\n        Returns:\n            dict: The response JSON from the model serving endpoint.\n\n        Raises:\n            requests.exceptions.HTTPError: If the request to the model serving endpoint fails.\n        \"\"\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.token}\",\n            \"Content-Type\": \"application/json\",\n        }\n        ds_dict = (\n            {\"dataframe_split\": dataset.to_dict(orient=\"split\")}\n            if isinstance(dataset, pd.DataFrame)\n            else self.create_tf_serving_json(dataset)\n        )\n        data_json = json.dumps(ds_dict, allow_nan=True)\n        response = requests.request(\n            method=\"POST\", headers=headers, url=self.url, data=data_json\n        )\n        if response.status_code != 200:\n            raise requests.exceptions.HTTPError(\n                f\"Request failed with status {response.status_code}, {response.text}\"\n            )\n        return response.json()\n</code></pre>"},{"location":"code-reference/ModelQuery/#src.fleming.discovery.model_query.ModelQuery.create_tf_serving_json","title":"<code>create_tf_serving_json(data)</code>","text":"<p>Creates a JSON object for TensorFlow serving.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[dict, DataFrame, ndarray]</code> <p>The input data.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The JSON object for TensorFlow serving.</p> Source code in <code>src/fleming/discovery/model_query.py</code> <pre><code>def create_tf_serving_json(self, data):\n    \"\"\"\n    Creates a JSON object for TensorFlow serving.\n\n    Parameters:\n        data (Union[dict, pd.DataFrame, np.ndarray]): The input data.\n\n    Returns:\n        dict: The JSON object for TensorFlow serving.\n    \"\"\"\n    return {\n        \"inputs\": (\n            {name: data[name].tolist() for name in data.keys()}\n            if isinstance(data, dict)\n            else data.tolist()\n        )\n    }\n</code></pre>"},{"location":"code-reference/ModelQuery/#src.fleming.discovery.model_query.ModelQuery.score_model","title":"<code>score_model(dataset)</code>","text":"<p>Scores the model using the provided dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[DataFrame, ndarray]</code> <p>The dataset to be scored.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The response JSON from the model serving endpoint.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the request to the model serving endpoint fails.</p> Source code in <code>src/fleming/discovery/model_query.py</code> <pre><code>def score_model(self, dataset):\n    \"\"\"\n    Scores the model using the provided dataset.\n\n    Parameters:\n        dataset (Union[pd.DataFrame, np.ndarray]): The dataset to be scored.\n\n    Returns:\n        dict: The response JSON from the model serving endpoint.\n\n    Raises:\n        requests.exceptions.HTTPError: If the request to the model serving endpoint fails.\n    \"\"\"\n    headers = {\n        \"Authorization\": f\"Bearer {self.token}\",\n        \"Content-Type\": \"application/json\",\n    }\n    ds_dict = (\n        {\"dataframe_split\": dataset.to_dict(orient=\"split\")}\n        if isinstance(dataset, pd.DataFrame)\n        else self.create_tf_serving_json(dataset)\n    )\n    data_json = json.dumps(ds_dict, allow_nan=True)\n    response = requests.request(\n        method=\"POST\", headers=headers, url=self.url, data=data_json\n    )\n    if response.status_code != 200:\n        raise requests.exceptions.HTTPError(\n            f\"Request failed with status {response.status_code}, {response.text}\"\n        )\n    return response.json()\n</code></pre>"},{"location":"code-reference/ModelServe/","title":"ModelServe","text":""},{"location":"code-reference/ModelServe/#src.fleming.discovery.model_serve.ModelServe","title":"<code>ModelServe</code>","text":"<p>A class which allows for creating a model serving endpoint on databricks.</p>"},{"location":"code-reference/ModelServe/#src.fleming.discovery.model_serve.ModelServe--example","title":"Example:","text":"<pre><code>from fleming.discovery.model_serve import ModelServe\nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"model_serving\").getOrCreate()\n\n# Set the name of the MLflow endpoint\nendpoint_name = \"aidiscoverytool\"\nprint(f'Endpoint name: {endpoint_name}')\n\n# Name of the registered MLflow model\nmodel_name = \"BERT_Semantic_Search\"\nprint(f'Model name: {model_name}')\n\n# Get the latest version of the MLflow model\nlatest_version = max(MlflowClient().get_latest_versions(model_name), key=lambda v: v.version)\nmodel_version = latest_version.version\nprint(f'Model version: {model_version}')\n\n# Specify the type of compute (CPU, GPU_SMALL, GPU_LARGE, etc.)\nworkload_type = \"CPU\"\nprint(f'Workload type: {workload_type}')\n\n# Specify the scale-out size of compute (Small, Medium, Large, etc.)\nworkload_size = \"Small\"\nprint(f'Workload size: {workload_size}')\n\n# Specify Scale to Zero(only supported for CPU endpoints)\nscale_to_zero = False\nprint(f'Scale to zero: {scale_to_zero}')\n\nAPI_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\nAPI_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n\nmodel_serve = ModelServe(endpoint_name, model_name, workload_type, workload_size, scale_to_zero, API_ROOT, API_TOKEN)\nmodel_serve.deploy_endpoint()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>The name of the model serving endpoint.</p> required <code>model_name</code> <code>str</code> <p>The name of the model to be served.</p> required <code>workload_type</code> <code>str</code> <p>The type of compute to be used for the endpoint.</p> required <code>workload_size</code> <code>str</code> <p>The scale-out size of the compute.</p> required <code>scale_to_zero</code> <code>bool</code> <p>Whether to scale the compute to zero when not in use.</p> required <code>API_ROOT</code> <code>str</code> <p>The API root of the Databricks workspace.</p> <code>None</code> <code>API_TOKEN</code> <code>str</code> <p>The API token of the Databricks workspace.</p> <code>None</code> Source code in <code>src/fleming/discovery/model_serve.py</code> <pre><code>class ModelServe:\n    \"\"\"\n    A class which allows for creating a model serving endpoint on databricks.\n\n    Example:\n    --------\n    ```python\n\n    from fleming.discovery.model_serve import ModelServe\n    from pyspark.sql import SparkSession\n\n    # Not required if using Databricks\n    spark = SparkSession.builder.appName(\"model_serving\").getOrCreate()\n\n    # Set the name of the MLflow endpoint\n    endpoint_name = \"aidiscoverytool\"\n    print(f'Endpoint name: {endpoint_name}')\n\n    # Name of the registered MLflow model\n    model_name = \"BERT_Semantic_Search\"\n    print(f'Model name: {model_name}')\n\n    # Get the latest version of the MLflow model\n    latest_version = max(MlflowClient().get_latest_versions(model_name), key=lambda v: v.version)\n    model_version = latest_version.version\n    print(f'Model version: {model_version}')\n\n    # Specify the type of compute (CPU, GPU_SMALL, GPU_LARGE, etc.)\n    workload_type = \"CPU\"\n    print(f'Workload type: {workload_type}')\n\n    # Specify the scale-out size of compute (Small, Medium, Large, etc.)\n    workload_size = \"Small\"\n    print(f'Workload size: {workload_size}')\n\n    # Specify Scale to Zero(only supported for CPU endpoints)\n    scale_to_zero = False\n    print(f'Scale to zero: {scale_to_zero}')\n\n    API_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n    API_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n\n    model_serve = ModelServe(endpoint_name, model_name, workload_type, workload_size, scale_to_zero, API_ROOT, API_TOKEN)\n    model_serve.deploy_endpoint()\n\n    ```\n\n    Parameters:\n        endpoint_name (str): The name of the model serving endpoint.\n        model_name (str): The name of the model to be served.\n        workload_type (str): The type of compute to be used for the endpoint.\n        workload_size (str): The scale-out size of the compute.\n        scale_to_zero (bool): Whether to scale the compute to zero when not in use.\n        API_ROOT (str): The API root of the Databricks workspace.\n        API_TOKEN (str): The API token of the Databricks workspace.\n    \"\"\"\n\n    spark: SparkSession\n    endpoint_name: str\n    model_name: str\n    workload_type: str\n    workload_size: str\n    scale_to_zero: bool\n    API_ROOT: str\n    API_TOKEN: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        endpoint_name: str,\n        model_name: str,\n        workload_type: str,\n        workload_size: str,\n        scale_to_zero: str,\n        API_ROOT: str = None,\n        API_TOKEN: str = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.endpoint_name = endpoint_name\n        self.model_name = model_name\n        self.workload_type = workload_type\n        self.workload_size = workload_size\n        self.scale_to_zero = scale_to_zero\n        self.API_ROOT = API_ROOT\n        self.API_TOKEN = API_TOKEN\n\n    def deploy_endpoint(self) -&gt; None:\n        \"\"\"\n        Create the model serving endpoint on Databricks\n\n        \"\"\"\n\n        try:\n            client = get_deploy_client(\"databricks\")\n            client.create_endpoint(\n                name=self.endpoint_name,\n                config={\n                    \"served_entities\": [\n                        {\n                            \"name\": self.model_name,\n                            \"entity_name\": self.model_name,\n                            \"entity_version\": MlflowClient()\n                            .get_registered_model(self.model_name)\n                            .latest_versions[1]\n                            .version,\n                            \"workload_type\": self.workload_type,\n                            \"workload_size\": self.workload_size,\n                            \"scale_to_zero_enabled\": self.scale_to_zero,\n                        }\n                    ],\n                    \"traffic_config\": {\n                        \"routes\": [\n                            {\n                                \"served_model_name\": self.model_name,\n                                \"traffic_percentage\": 100,\n                            }\n                        ]\n                    },\n                },\n            )\n        except requests.exceptions.RequestException as e:\n            put_url = \"/api/2.0/serving-endpoints/{}/config\".format(self.endpoint_name)\n            put_url\n\n            data = {\n                \"name\": self.endpoint_name,\n                \"config\": {\n                    \"served_entities\": [\n                        {\n                            \"name\": self.model_name,\n                            \"entity_name\": self.model_name,\n                            \"entity_version\": max(\n                                MlflowClient().get_latest_versions(self.model_name),\n                                key=lambda v: v.version,\n                            ).version,\n                            \"workload_type\": self.workload_type,\n                            \"workload_size\": self.workload_size,\n                            \"scale_to_zero_enabled\": self.scale_to_zero,\n                        }\n                    ],\n                    \"traffic_config\": {\n                        \"routes\": [\n                            {\n                                \"served_model_name\": self.model_name,\n                                \"traffic_percentage\": 100,\n                            }\n                        ]\n                    },\n                },\n            }\n\n            headers = {\n                \"Context-Type\": \"text/json\",\n                \"Authorization\": f\"Bearer {self.API_TOKEN}\",\n            }\n\n            response = requests.put(\n                url=f\"{self.API_ROOT}{put_url}\", json=data[\"config\"], headers=headers\n            )\n\n            if response.status_code != 200:\n                raise requests.exceptions.RequestException(\n                    f\"Request failed with status {response.status_code}, {response.text}\"\n                )\n\n            return response.json()\n            raise\n</code></pre>"},{"location":"code-reference/ModelServe/#src.fleming.discovery.model_serve.ModelServe.deploy_endpoint","title":"<code>deploy_endpoint()</code>","text":"<p>Create the model serving endpoint on Databricks</p> Source code in <code>src/fleming/discovery/model_serve.py</code> <pre><code>def deploy_endpoint(self) -&gt; None:\n    \"\"\"\n    Create the model serving endpoint on Databricks\n\n    \"\"\"\n\n    try:\n        client = get_deploy_client(\"databricks\")\n        client.create_endpoint(\n            name=self.endpoint_name,\n            config={\n                \"served_entities\": [\n                    {\n                        \"name\": self.model_name,\n                        \"entity_name\": self.model_name,\n                        \"entity_version\": MlflowClient()\n                        .get_registered_model(self.model_name)\n                        .latest_versions[1]\n                        .version,\n                        \"workload_type\": self.workload_type,\n                        \"workload_size\": self.workload_size,\n                        \"scale_to_zero_enabled\": self.scale_to_zero,\n                    }\n                ],\n                \"traffic_config\": {\n                    \"routes\": [\n                        {\n                            \"served_model_name\": self.model_name,\n                            \"traffic_percentage\": 100,\n                        }\n                    ]\n                },\n            },\n        )\n    except requests.exceptions.RequestException as e:\n        put_url = \"/api/2.0/serving-endpoints/{}/config\".format(self.endpoint_name)\n        put_url\n\n        data = {\n            \"name\": self.endpoint_name,\n            \"config\": {\n                \"served_entities\": [\n                    {\n                        \"name\": self.model_name,\n                        \"entity_name\": self.model_name,\n                        \"entity_version\": max(\n                            MlflowClient().get_latest_versions(self.model_name),\n                            key=lambda v: v.version,\n                        ).version,\n                        \"workload_type\": self.workload_type,\n                        \"workload_size\": self.workload_size,\n                        \"scale_to_zero_enabled\": self.scale_to_zero,\n                    }\n                ],\n                \"traffic_config\": {\n                    \"routes\": [\n                        {\n                            \"served_model_name\": self.model_name,\n                            \"traffic_percentage\": 100,\n                        }\n                    ]\n                },\n            },\n        }\n\n        headers = {\n            \"Context-Type\": \"text/json\",\n            \"Authorization\": f\"Bearer {self.API_TOKEN}\",\n        }\n\n        response = requests.put(\n            url=f\"{self.API_ROOT}{put_url}\", json=data[\"config\"], headers=headers\n        )\n\n        if response.status_code != 200:\n            raise requests.exceptions.RequestException(\n                f\"Request failed with status {response.status_code}, {response.text}\"\n            )\n\n        return response.json()\n        raise\n</code></pre>"},{"location":"code-reference/ModelServe/#src.fleming.discovery.model_serve.ModelServewithMosaicAI","title":"<code>ModelServewithMosaicAI</code>","text":"<p>A class which allows for creating a model serving endpoint on databricks with Mosaic AI.</p>"},{"location":"code-reference/ModelServe/#src.fleming.discovery.model_serve.ModelServewithMosaicAI--example","title":"Example:","text":"<pre><code>from fleming.discovery.model_serve import ModelServewithMosaicAI\nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"model_serving\").getOrCreate()\n\n# Set the name of the MLflow endpoint\nendpoint_name = \"aidiscoverytool\"\nprint(f'Endpoint name: {endpoint_name}')\n\n# Name of the registered MLflow model\nmodel_name = \"BERT_Semantic_Search\"\nprint(f'Model name: {model_name}')\n\n# Get the latest version of the MLflow model\nlatest_version = max(MlflowClient().get_latest_versions(model_name), key=lambda v: v.version)\nmodel_version = latest_version.version\nprint(f'Model version: {model_version}')\n\n# Specify the type of compute (CPU, GPU_SMALL, GPU_LARGE, etc.)\nworkload_type = \"CPU\"\nprint(f'Workload type: {workload_type}')\n\n# Specify the scale-out size of compute (Small, Medium, Large, etc.)\nworkload_size = \"Small\"\nprint(f'Workload size: {workload_size}')\n\n# Specify Scale to Zero(only supported for CPU endpoints)\nscale_to_zero = False\nprint(f'Scale to zero: {scale_to_zero}')\n\nAPI_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\nAPI_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n\nmodel_serve = ModelServewithMosaicAI(endpoint_name, model_name, workload_type, workload_size, scale_to_zero, API_ROOT, API_TOKEN)\nmodel_serve.deploy_endpoint()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>The name of the model serving endpoint.</p> required <code>model_name</code> <code>str</code> <p>The name of the model to be served.</p> required <code>workload_type</code> <code>str</code> <p>The type of compute to be used for the endpoint.</p> required <code>workload_size</code> <code>str</code> <p>The scale-out size of the compute.</p> required <code>scale_to_zero</code> <code>bool</code> <p>Whether to scale the compute to zero when not in use.</p> required <code>API_ROOT</code> <code>str</code> <p>The API root of the Databricks workspace.</p> <code>None</code> <code>API_TOKEN</code> <code>str</code> <p>The API token of the Databricks workspace.</p> <code>None</code> <code>ebable_usage_logging</code> <code>bool</code> <p>Whether to enable usage logging.</p> required <code>enable_payload_logging</code> <code>bool</code> <p>Whether to enable payload logging.</p> <code>True</code> <code>rate_limit_rps</code> <code>int</code> <p>The rate limit in requests per second.</p> <code>10</code> <code>rate_limit_concurrent</code> <code>int</code> <p>The rate limit for concurrent requests.</p> <code>5</code> <code>access_control_enabled</code> <code>bool</code> <p>Whether to enable access control.</p> <code>False</code> <code>allowed_user_ids</code> <code>list</code> <p>List of user IDs allowed to access the endpoint.</p> <code>None</code> Source code in <code>src/fleming/discovery/model_serve.py</code> <pre><code>class ModelServewithMosaicAI:\n    \"\"\"\n    A class which allows for creating a model serving endpoint on databricks with Mosaic AI.\n\n    Example:\n    --------\n    ```python\n\n    from fleming.discovery.model_serve import ModelServewithMosaicAI\n    from pyspark.sql import SparkSession\n\n    # Not required if using Databricks\n    spark = SparkSession.builder.appName(\"model_serving\").getOrCreate()\n\n    # Set the name of the MLflow endpoint\n    endpoint_name = \"aidiscoverytool\"\n    print(f'Endpoint name: {endpoint_name}')\n\n    # Name of the registered MLflow model\n    model_name = \"BERT_Semantic_Search\"\n    print(f'Model name: {model_name}')\n\n    # Get the latest version of the MLflow model\n    latest_version = max(MlflowClient().get_latest_versions(model_name), key=lambda v: v.version)\n    model_version = latest_version.version\n    print(f'Model version: {model_version}')\n\n    # Specify the type of compute (CPU, GPU_SMALL, GPU_LARGE, etc.)\n    workload_type = \"CPU\"\n    print(f'Workload type: {workload_type}')\n\n    # Specify the scale-out size of compute (Small, Medium, Large, etc.)\n    workload_size = \"Small\"\n    print(f'Workload size: {workload_size}')\n\n    # Specify Scale to Zero(only supported for CPU endpoints)\n    scale_to_zero = False\n    print(f'Scale to zero: {scale_to_zero}')\n\n    API_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n    API_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n\n    model_serve = ModelServewithMosaicAI(endpoint_name, model_name, workload_type, workload_size, scale_to_zero, API_ROOT, API_TOKEN)\n    model_serve.deploy_endpoint()\n\n    ```\n\n    Parameters:\n        endpoint_name (str): The name of the model serving endpoint.\n        model_name (str): The name of the model to be served.\n        workload_type (str): The type of compute to be used for the endpoint.\n        workload_size (str): The scale-out size of the compute.\n        scale_to_zero (bool): Whether to scale the compute to zero when not in use.\n        API_ROOT (str): The API root of the Databricks workspace.\n        API_TOKEN (str): The API token of the Databricks workspace.\n        ebable_usage_logging (bool): Whether to enable usage logging.\n        enable_payload_logging (bool): Whether to enable payload logging.\n        rate_limit_rps (int): The rate limit in requests per second.\n        rate_limit_concurrent (int): The rate limit for concurrent requests.\n        access_control_enabled (bool): Whether to enable access control.\n        allowed_user_ids (list): List of user IDs allowed to access the endpoint.\n    \"\"\"\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        endpoint_name: str,\n        model_name: str,\n        workload_type: str,\n        workload_size: str,\n        scale_to_zero: bool,\n        API_ROOT: str = None,\n        API_TOKEN: str = None,\n        enable_usage_logging: bool = True,\n        enable_payload_logging: bool = True,\n        rate_limit_rps: int = 10,\n        rate_limit_concurrent: int = 5,\n        access_control_enabled: bool = False,\n        allowed_user_ids: list = None,\n    ) -&gt; None:\n        self.spark = spark\n        self.endpoint_name = endpoint_name\n        self.model_name = model_name\n        self.workload_type = workload_type\n        self.workload_size = workload_size\n        self.scale_to_zero = scale_to_zero\n        self.API_ROOT = API_ROOT\n        self.API_TOKEN = API_TOKEN\n        # Mosaic AI Gateway settings\n        self.enable_usage_logging = enable_usage_logging\n        self.enable_payload_logging = enable_payload_logging\n        self.rate_limit_rps = rate_limit_rps\n        self.rate_limit_concurrent = rate_limit_concurrent\n        self.access_control_enabled = access_control_enabled\n        self.allowed_user_ids = allowed_user_ids or []\n\n    def _build_config(self):\n        return {\n            \"served_entities\": [\n                {\n                    \"name\": self.model_name,\n                    \"entity_name\": self.model_name,\n                    \"entity_version\": max(\n                        MlflowClient().get_latest_versions(self.model_name),\n                        key=lambda v: v.version,\n                    ).version,\n                    \"workload_type\": self.workload_type,\n                    \"workload_size\": self.workload_size,\n                    \"scale_to_zero_enabled\": self.scale_to_zero,\n                }\n            ],\n            \"traffic_config\": {\n                \"routes\": [\n                    {\n                        \"served_model_name\": self.model_name,\n                        \"traffic_percentage\": 100,\n                    }\n                ]\n            },\n            \"ai_gateway_config\": {\n                \"usage_logging_enabled\": self.enable_usage_logging,\n                \"payload_logging_enabled\": self.enable_payload_logging,\n                \"rate_limit\": {\n                    \"requests_per_second\": self.rate_limit_rps,\n                    \"concurrent_requests\": self.rate_limit_concurrent,\n                },\n                \"access_control\": {\n                    \"enabled\": self.access_control_enabled,\n                    \"allowed_user_ids\": self.allowed_user_ids,\n                },\n            },\n        }\n\n    def deploy_endpoint(self) -&gt; None:\n        config = self._build_config()\n\n        try:\n            client = get_deploy_client(\"databricks\")\n            client.create_endpoint(\n                name=self.endpoint_name,\n                config=config,\n            )\n        except requests.exceptions.RequestException as e:\n            put_url = f\"/api/2.0/serving-endpoints/{self.endpoint_name}/config\"\n            headers = {\n                \"Content-Type\": \"application/json\",\n                \"Authorization\": f\"Bearer {self.API_TOKEN}\",\n            }\n\n            response = requests.put(\n                url=f\"{self.API_ROOT}{put_url}\", json=config, headers=headers\n            )\n\n            if response.status_code != 200:\n                raise requests.exceptions.RequestException(\n                    f\"Request failed with status {response.status_code}, {response.text}\"\n                )\n\n            return response.json()\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/","title":"ModelTrainRegister","text":""},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.SemanticSearchModel","title":"<code>SemanticSearchModel</code>","text":"<p>               Bases: <code>PythonModel</code></p> <p>A class representing a semantic search model.</p> <p>This class is used to perform semantic search over a corpus of sentences using a pre-trained model.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The pre-trained model used for encoding sentences.</p> <code>corpus</code> <p>The corpus of sentences used for semantic search.</p> <code>corpus_embeddings</code> <p>The embeddings of the sentences in the corpus.</p> <p>Methods:</p> Name Description <code>load_context</code> <p>Load the model context for inference, including the corpus from a file.</p> <code>predict</code> <p>Perform semantic search over the corpus and return the most relevant results.</p> Source code in <code>src/fleming/discovery/model_train_register.py</code> <pre><code>class SemanticSearchModel(PythonModel):\n    \"\"\"\n    A class representing a semantic search model.\n\n    This class is used to perform semantic search over a corpus of sentences using a pre-trained model.\n\n    Attributes:\n        model: The pre-trained model used for encoding sentences.\n        corpus: The corpus of sentences used for semantic search.\n        corpus_embeddings: The embeddings of the sentences in the corpus.\n\n    Methods:\n        load_context: Load the model context for inference, including the corpus from a file.\n        predict: Perform semantic search over the corpus and return the most relevant results.\n    \"\"\"\n\n    def load_context(self, context):\n        \"\"\"\n        Load the model context for inference, including the corpus from a file.\n        \"\"\"\n        try:\n            self.model = SentenceTransformer.load(context.artifacts[\"model_path\"])\n\n            # Load the corpus from the specified file\n            corpus_file = context.artifacts[\"corpus_file\"]\n            with open(corpus_file) as file:\n                self.corpus = file.read().splitlines()\n\n            self.corpus_embeddings = torch.load(\n                context.artifacts[\"corpus_embedding_file\"]\n            )\n\n        except Exception as e:\n            raise ValueError(f\"Error loading model and corpus: {e}\")\n\n    def predict(self, context, model_input, params=None):\n        \"\"\"\n        Predict method to perform semantic search over the corpus.\n\n        Args:\n            context: The context object containing the model artifacts.\n            model_input: The input data for performing semantic search.\n            params: Optional parameters for controlling the search behavior.\n\n        Returns:\n            A list of tuples containing the most relevant sentences from the corpus and their similarity scores.\n        \"\"\"\n\n        if isinstance(model_input, pd.DataFrame):\n            if model_input.shape[1] != 1:\n                raise ValueError(\"DataFrame input must have exactly one column.\")\n            model_input = model_input.iloc[0, 0]\n        elif isinstance(model_input, dict):\n            model_input = model_input.get(\"sentence\")\n            if model_input is None:\n                raise ValueError(\n                    \"The input dictionary must have a key named 'sentence'.\"\n                )\n        else:\n            raise TypeError(\n                f\"Unexpected type for model_input: {type(model_input)}. Must be either a Dict or a DataFrame.\"\n            )\n\n        # Encode the query\n        query_embedding = self.model.encode(model_input, convert_to_tensor=True)\n\n        # Compute cosine similarity scores\n        cos_scores = util.cos_sim(query_embedding, self.corpus_embeddings).cpu()[0]\n\n        # Determine the number of top results to return\n        top_k = params.get(\"top_k\", 3) if params else 3  # Default to 3 if not specified\n\n        _ = (\n            params.get(\"minimum_relevancy\", 0.4) if params else 0.4\n        )  # Default to 0.4 if not specified\n\n        # Get the top_k most similar sentences from the corpus\n        top_results = np.argsort(cos_scores, axis=0)[-top_k:]\n\n        # Prepare the initial results list\n        initial_results = [\n            (self.corpus[idx], cos_scores[idx].item()) for idx in reversed(top_results)\n        ]\n\n        # Filter the results based on the minimum relevancy threshold\n        filtered_results = [result for result in initial_results if result[1] &gt;= 0]\n\n        # If all results are below the threshold, issue a warning and return the top result\n        if not filtered_results:\n            warnings.warn(\n                \"All top results are below the minimum relevancy threshold. \"\n                \"Returning the highest match instead.\",\n                RuntimeWarning,\n            )\n            return [initial_results[0]]\n        else:\n            return filtered_results\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.SemanticSearchModel.load_context","title":"<code>load_context(context)</code>","text":"<p>Load the model context for inference, including the corpus from a file.</p> Source code in <code>src/fleming/discovery/model_train_register.py</code> <pre><code>def load_context(self, context):\n    \"\"\"\n    Load the model context for inference, including the corpus from a file.\n    \"\"\"\n    try:\n        self.model = SentenceTransformer.load(context.artifacts[\"model_path\"])\n\n        # Load the corpus from the specified file\n        corpus_file = context.artifacts[\"corpus_file\"]\n        with open(corpus_file) as file:\n            self.corpus = file.read().splitlines()\n\n        self.corpus_embeddings = torch.load(\n            context.artifacts[\"corpus_embedding_file\"]\n        )\n\n    except Exception as e:\n        raise ValueError(f\"Error loading model and corpus: {e}\")\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.SemanticSearchModel.predict","title":"<code>predict(context, model_input, params=None)</code>","text":"<p>Predict method to perform semantic search over the corpus.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>The context object containing the model artifacts.</p> required <code>model_input</code> <p>The input data for performing semantic search.</p> required <code>params</code> <p>Optional parameters for controlling the search behavior.</p> <code>None</code> <p>Returns:</p> Type Description <p>A list of tuples containing the most relevant sentences from the corpus and their similarity scores.</p> Source code in <code>src/fleming/discovery/model_train_register.py</code> <pre><code>def predict(self, context, model_input, params=None):\n    \"\"\"\n    Predict method to perform semantic search over the corpus.\n\n    Args:\n        context: The context object containing the model artifacts.\n        model_input: The input data for performing semantic search.\n        params: Optional parameters for controlling the search behavior.\n\n    Returns:\n        A list of tuples containing the most relevant sentences from the corpus and their similarity scores.\n    \"\"\"\n\n    if isinstance(model_input, pd.DataFrame):\n        if model_input.shape[1] != 1:\n            raise ValueError(\"DataFrame input must have exactly one column.\")\n        model_input = model_input.iloc[0, 0]\n    elif isinstance(model_input, dict):\n        model_input = model_input.get(\"sentence\")\n        if model_input is None:\n            raise ValueError(\n                \"The input dictionary must have a key named 'sentence'.\"\n            )\n    else:\n        raise TypeError(\n            f\"Unexpected type for model_input: {type(model_input)}. Must be either a Dict or a DataFrame.\"\n        )\n\n    # Encode the query\n    query_embedding = self.model.encode(model_input, convert_to_tensor=True)\n\n    # Compute cosine similarity scores\n    cos_scores = util.cos_sim(query_embedding, self.corpus_embeddings).cpu()[0]\n\n    # Determine the number of top results to return\n    top_k = params.get(\"top_k\", 3) if params else 3  # Default to 3 if not specified\n\n    _ = (\n        params.get(\"minimum_relevancy\", 0.4) if params else 0.4\n    )  # Default to 0.4 if not specified\n\n    # Get the top_k most similar sentences from the corpus\n    top_results = np.argsort(cos_scores, axis=0)[-top_k:]\n\n    # Prepare the initial results list\n    initial_results = [\n        (self.corpus[idx], cos_scores[idx].item()) for idx in reversed(top_results)\n    ]\n\n    # Filter the results based on the minimum relevancy threshold\n    filtered_results = [result for result in initial_results if result[1] &gt;= 0]\n\n    # If all results are below the threshold, issue a warning and return the top result\n    if not filtered_results:\n        warnings.warn(\n            \"All top results are below the minimum relevancy threshold. \"\n            \"Returning the highest match instead.\",\n            RuntimeWarning,\n        )\n        return [initial_results[0]]\n    else:\n        return filtered_results\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister","title":"<code>ModelTrainRegister</code>","text":"<p>A class to train and register a semantic search model.</p>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister--example","title":"Example:","text":"<pre><code>from fleming.discovery.model_train_register import ModelTrainRegister, SemanticSearchModel\nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"model_serving\").getOrCreate()\n\nmodel_directory = \"/tmp/BERT_Semantic_Search_model\"\ncorpus_file = \"/tmp/search_corpus.txt\"\ncorpus_embedding_file = '/tmp/corpus_embedding.pt'\n\nmodel_developer = ModelTrainRegister(spark, model_directory, corpus_file, corpus_embedding_file)\n\n# Register the model\nsemantic_search_model = \"multi-qa-mpnet-base-dot-v1\"\nmodel_developer.register_model(semantic_search_model)\n\n# Embed the corpus\nmodel_developer.embed_corpus()\n\n# Define parameters and artifacts\nparameters = {\"top_k\": 50, \"relevancy_score\": 0.45}\ninput_example = [\"Innersource best practices\"]\ntest_output = [\"match 1\", \"match 2\"]\nsignature = infer_signature(input_example, test_output, params=parameters)\nartifacts = {\n    \"model_path\": model_directory,\n    \"corpus_file\": corpus_file,\n    \"corpus_embedding_file\": corpus_embedding_file\n}\nunique_model_name = \"semantic_search_model\"\n\n# Create and serve the model\nexperiment_location = \"/path/to/experiment\"\nmodel_developer.create_registered_model(unique_model_name, input_example, signature, artifacts, experiment_location)\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister--parameters","title":"Parameters:","text":"<p>model_directory (str): The directory to save the trained model. corpus_file (str): The file containing the corpus of sentences. corpus_embedding_file (str): The file to save the embeddings of the corpus. semantic_search_model (str): The pre-trained model to use for semantic search.</p> Source code in <code>src/fleming/discovery/model_train_register.py</code> <pre><code>class ModelTrainRegister:\n    \"\"\"\n    A class to train and register a semantic search model.\n\n    Example:\n    --------\n    ```python\n\n    from fleming.discovery.model_train_register import ModelTrainRegister, SemanticSearchModel\n    from pyspark.sql import SparkSession\n\n    # Not required if using Databricks\n    spark = SparkSession.builder.appName(\"model_serving\").getOrCreate()\n\n    model_directory = \"/tmp/BERT_Semantic_Search_model\"\n    corpus_file = \"/tmp/search_corpus.txt\"\n    corpus_embedding_file = '/tmp/corpus_embedding.pt'\n\n    model_developer = ModelTrainRegister(spark, model_directory, corpus_file, corpus_embedding_file)\n\n    # Register the model\n    semantic_search_model = \"multi-qa-mpnet-base-dot-v1\"\n    model_developer.register_model(semantic_search_model)\n\n    # Embed the corpus\n    model_developer.embed_corpus()\n\n    # Define parameters and artifacts\n    parameters = {\"top_k\": 50, \"relevancy_score\": 0.45}\n    input_example = [\"Innersource best practices\"]\n    test_output = [\"match 1\", \"match 2\"]\n    signature = infer_signature(input_example, test_output, params=parameters)\n    artifacts = {\n        \"model_path\": model_directory,\n        \"corpus_file\": corpus_file,\n        \"corpus_embedding_file\": corpus_embedding_file\n    }\n    unique_model_name = \"semantic_search_model\"\n\n    # Create and serve the model\n    experiment_location = \"/path/to/experiment\"\n    model_developer.create_registered_model(unique_model_name, input_example, signature, artifacts, experiment_location)\n    ```\n\n    Parameters:\n    -----------\n    model_directory (str): The directory to save the trained model.\n    corpus_file (str): The file containing the corpus of sentences.\n    corpus_embedding_file (str): The file to save the embeddings of the corpus.\n    semantic_search_model (str): The pre-trained model to use for semantic search.\n    \"\"\"\n\n    spark: SparkSession\n    model_directory: str\n    corpus_file: str\n    corpus_embedding_file: str\n    semantic_search_model: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        model_directory: str,\n        corpus_file: str,\n        corpus_embedding_file: str,\n        semantic_search_model: str,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ModelDeveloper class.\n\n        Parameters:\n        -----------\n        spark : SparkSession\n        model_directory : str\n            The directory to save the trained model.\n        corpus_file : str\n            The file containing the corpus of sentences.\n        corpus_embedding_file : str\n            The file to save the embeddings of the corpus.\n        semantic_search_model : str\n            The pre-trained model to use for semantic search.\n        \"\"\"\n        self.model_directory = model_directory\n        self.corpus_file = corpus_file\n        self.corpus_embedding_file = corpus_embedding_file\n        self.semantic_search_model = semantic_search_model\n\n    def register_model(self) -&gt; None:\n        \"\"\"\n        Register the pre-trained model.\n\n        \"\"\"\n        model = SentenceTransformer(self.semantic_search_model)\n        model.save(self.model_directory)\n\n    def embed_corpus(self) -&gt; None:\n        \"\"\"\n        Embed the corpus of sentences using the pre-trained model.\n        \"\"\"\n        model = SentenceTransformer.load(self.model_directory)\n\n        with open(self.corpus_file) as file:\n            corpus = file.read().splitlines()\n\n        corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n        torch.save(corpus_embeddings, self.corpus_embedding_file)\n\n    def create_registered_model(\n        self,\n        unique_model_name: str,\n        input_example: list,\n        signature: object,\n        artifacts: str,\n        experiment_location: str,\n    ) -&gt; None:\n        \"\"\"\n        Create and serve the semantic search model.\n\n        Parameters:\n        -----------\n        unique_model_name : str\n            The unique name for the model.\n        input_example : list\n            An example input for the model.\n        signature : object\n            The signature object for the model.\n        artifacts : dict\n            The artifacts required for the model.\n        experiment_location : str\n            The location to store the experiment.\n        \"\"\"\n        mlflow.set_experiment(experiment_location)\n\n        with mlflow.start_run() as run:\n            model_info = mlflow.pyfunc.log_model(\n                unique_model_name,\n                python_model=SemanticSearchModel(),\n                input_example=input_example,\n                signature=signature,\n                artifacts=artifacts,\n                pip_requirements=[\"sentence_transformers\", \"numpy\"],\n            )\n            _ = run.info.run_id\n\n            model_uri = model_info.model_uri\n\n            mlflow.register_model(model_uri=model_uri, name=unique_model_name)\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister.__init__","title":"<code>__init__(spark, model_directory, corpus_file, corpus_embedding_file, semantic_search_model)</code>","text":"<p>Initialize the ModelDeveloper class.</p>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister.__init__--parameters","title":"Parameters:","text":"<p>spark : SparkSession model_directory : str     The directory to save the trained model. corpus_file : str     The file containing the corpus of sentences. corpus_embedding_file : str     The file to save the embeddings of the corpus. semantic_search_model : str     The pre-trained model to use for semantic search.</p> Source code in <code>src/fleming/discovery/model_train_register.py</code> <pre><code>def __init__(\n    self,\n    spark: SparkSession,\n    model_directory: str,\n    corpus_file: str,\n    corpus_embedding_file: str,\n    semantic_search_model: str,\n) -&gt; None:\n    \"\"\"\n    Initialize the ModelDeveloper class.\n\n    Parameters:\n    -----------\n    spark : SparkSession\n    model_directory : str\n        The directory to save the trained model.\n    corpus_file : str\n        The file containing the corpus of sentences.\n    corpus_embedding_file : str\n        The file to save the embeddings of the corpus.\n    semantic_search_model : str\n        The pre-trained model to use for semantic search.\n    \"\"\"\n    self.model_directory = model_directory\n    self.corpus_file = corpus_file\n    self.corpus_embedding_file = corpus_embedding_file\n    self.semantic_search_model = semantic_search_model\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister.register_model","title":"<code>register_model()</code>","text":"<p>Register the pre-trained model.</p> Source code in <code>src/fleming/discovery/model_train_register.py</code> <pre><code>def register_model(self) -&gt; None:\n    \"\"\"\n    Register the pre-trained model.\n\n    \"\"\"\n    model = SentenceTransformer(self.semantic_search_model)\n    model.save(self.model_directory)\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister.embed_corpus","title":"<code>embed_corpus()</code>","text":"<p>Embed the corpus of sentences using the pre-trained model.</p> Source code in <code>src/fleming/discovery/model_train_register.py</code> <pre><code>def embed_corpus(self) -&gt; None:\n    \"\"\"\n    Embed the corpus of sentences using the pre-trained model.\n    \"\"\"\n    model = SentenceTransformer.load(self.model_directory)\n\n    with open(self.corpus_file) as file:\n        corpus = file.read().splitlines()\n\n    corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n    torch.save(corpus_embeddings, self.corpus_embedding_file)\n</code></pre>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister.create_registered_model","title":"<code>create_registered_model(unique_model_name, input_example, signature, artifacts, experiment_location)</code>","text":"<p>Create and serve the semantic search model.</p>"},{"location":"code-reference/ModelTrainRegister/#src.fleming.discovery.model_train_register.ModelTrainRegister.create_registered_model--parameters","title":"Parameters:","text":"<p>unique_model_name : str     The unique name for the model. input_example : list     An example input for the model. signature : object     The signature object for the model. artifacts : dict     The artifacts required for the model. experiment_location : str     The location to store the experiment.</p> Source code in <code>src/fleming/discovery/model_train_register.py</code> <pre><code>def create_registered_model(\n    self,\n    unique_model_name: str,\n    input_example: list,\n    signature: object,\n    artifacts: str,\n    experiment_location: str,\n) -&gt; None:\n    \"\"\"\n    Create and serve the semantic search model.\n\n    Parameters:\n    -----------\n    unique_model_name : str\n        The unique name for the model.\n    input_example : list\n        An example input for the model.\n    signature : object\n        The signature object for the model.\n    artifacts : dict\n        The artifacts required for the model.\n    experiment_location : str\n        The location to store the experiment.\n    \"\"\"\n    mlflow.set_experiment(experiment_location)\n\n    with mlflow.start_run() as run:\n        model_info = mlflow.pyfunc.log_model(\n            unique_model_name,\n            python_model=SemanticSearchModel(),\n            input_example=input_example,\n            signature=signature,\n            artifacts=artifacts,\n            pip_requirements=[\"sentence_transformers\", \"numpy\"],\n        )\n        _ = run.info.run_id\n\n        model_uri = model_info.model_uri\n\n        mlflow.register_model(model_uri=model_uri, name=unique_model_name)\n</code></pre>"},{"location":"code-reference/RepoContentsTokenisation/","title":"RepoContentsTokenisation","text":""},{"location":"code-reference/RepoContentsTokenisation/#src.fleming.code_summary.repo_contents_tokenisation.GitHubRepoDataProcessor","title":"<code>GitHubRepoDataProcessor</code>","text":"<p>Class to process data from GitHub repositories. This class ingests the files from a list of repositories in an organization and processes the contents to return a dataframe of the contents concatenated.</p> <p>The concatenated data can then be ingest into a model for further processing. For example, the concatenated data can be used to train a model for code summarization.</p> <p>The class contains the following methods:</p> <ol> <li>get_token: Get the GitHub token for the GitHub App.</li> <li>num_tokens_from_string: Get the number of tokens from a string based on the encoding.</li> <li>nlp_process: Process the HTML content to remove unwanted characters.</li> <li>get_directory_level: Get the directory level of a file.</li> <li>calculate_token_count: Calculate the token count of a text.</li> <li>get_new_github_instance: Get a new GitHub instance.</li> <li>get_organization_repos_data: Get the data from the repositories in the organization.</li> <li>data_collection: Collect repostiory contents from the repositories in the organization and return as a dataframe.</li> <li>concatenate_repo_contents: Concatenate the contents of the repositories into a dataframe.</li> </ol>"},{"location":"code-reference/RepoContentsTokenisation/#src.fleming.code_summary.repo_contents_tokenisation.GitHubRepoDataProcessor--example","title":"Example","text":"<pre><code>from fleming.code_summary.repo_contents_tokenisation import GitHubRepoDataProcessor\nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"RepoConcat\").getOrCreate()\n\norganization_name = 'company-x'\nrepo_list = ['repo1', 'repo2', 'repo3']\nnum_token_per_repo = 100000\npem_key = 'xxxxx'\npem_file = '/dbfs/FileStore/github_app/pem_key.pem'\n\ngithub_repo_data_processor = GitHubRepoDataProcessor(spark, organization_name, repo_list, num_token_per_repo, pem_key, pem_file)\nrepo_contents_df = github_repo_data_processor.data_collection()\nrepo_contents_df_concat = github_repo_data_processor.concatenate_repo_contents(repo_contents_df)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark Session</p> required <code>organization_name</code> <code>str</code> <p>Name of the organization in github</p> required <code>repo_list</code> <code>list</code> <p>List of repositories</p> required <code>num_token_per_repo</code> <code>int</code> <p>Max number of tokens per repository to be collected</p> required <code>pem_key</code> <code>str</code> <p>PEM key for the GitHub App</p> required <code>pem_file</code> <code>str</code> <p>Filepath to store the PEM key</p> required Source code in <code>src/fleming/code_summary/repo_contents_tokenisation.py</code> <pre><code>class GitHubRepoDataProcessor:\n    \"\"\"\n    Class to process data from GitHub repositories. This class ingests the files from a list of repositories in an organization and processes the contents to return a dataframe of the contents concatenated.\n\n    The concatenated data can then be ingest into a model for further processing. For example, the concatenated data can be used to train a model for code summarization.\n\n    The class contains the following methods:\n\n    1. get_token: Get the GitHub token for the GitHub App.\n    2. num_tokens_from_string: Get the number of tokens from a string based on the encoding.\n    3. nlp_process: Process the HTML content to remove unwanted characters.\n    4. get_directory_level: Get the directory level of a file.\n    5. calculate_token_count: Calculate the token count of a text.\n    6. get_new_github_instance: Get a new GitHub instance.\n    7. get_organization_repos_data: Get the data from the repositories in the organization.\n    8. data_collection: Collect repostiory contents from the repositories in the organization and return as a dataframe.\n    9. concatenate_repo_contents: Concatenate the contents of the repositories into a dataframe.\n\n\n    Example\n    --------\n    ```python\n\n    from fleming.code_summary.repo_contents_tokenisation import GitHubRepoDataProcessor\n    from pyspark.sql import SparkSession\n\n    # Not required if using Databricks\n    spark = SparkSession.builder.appName(\"RepoConcat\").getOrCreate()\n\n    organization_name = 'company-x'\n    repo_list = ['repo1', 'repo2', 'repo3']\n    num_token_per_repo = 100000\n    pem_key = 'xxxxx'\n    pem_file = '/dbfs/FileStore/github_app/pem_key.pem'\n\n    github_repo_data_processor = GitHubRepoDataProcessor(spark, organization_name, repo_list, num_token_per_repo, pem_key, pem_file)\n    repo_contents_df = github_repo_data_processor.data_collection()\n    repo_contents_df_concat = github_repo_data_processor.concatenate_repo_contents(repo_contents_df)\n\n    ```\n\n    Parameters:\n        spark (SparkSession): Spark Session\n        organization_name (str): Name of the organization in github\n        repo_list (list): List of repositories\n        num_token_per_repo (int): Max number of tokens per repository to be collected\n        pem_key (str): PEM key for the GitHub App\n        pem_file (str): Filepath to store the PEM key\n\n    \"\"\"\n\n    spark: SparkSession\n    organization_name: str\n    repo_list: list\n    num_token_per_repo: int\n    pem_key: str\n    pem_file: str\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        organization_name: str,\n        repo_list: list,\n        num_token_per_repo: int,\n        pem_key,\n        pem_file,\n    ):\n        self.spark = spark\n        self.organization_name = organization_name\n        self.repo_list = repo_list\n        self.num_token_per_repo = num_token_per_repo\n        self.pem_key = pem_key\n        self.pem_file = pem_file\n        self.ignore_files = [\n            \"__init__.py\",\n            \"LICENSE\",\n            \"LICENSE.md\",\n            \"LICENCE\",\n            \"LICENCE.md\",\n            \"CODEOWNERS.md\",\n            \".gitignore\",\n            \".gitattributes\",\n            \".gitmodules\",\n            \".git\",\n            \".DS_Store\",\n            \"Thumbs.db\",\n            \"CODE_OF_CONDUCT.md\",\n            \"CONTRIBUTING.md\",\n            \"SECURITY.md\",\n            \"PULL_REQUEST_TEMPLATE.md\",\n            \"ISSUE_TEMPLATE.md\",\n            \"MAINTAINERS.md\",\n            \"GOVERNANCE.md\",\n            \"RELEASE.md\",\n            \"SUPPORT.md\",\n            \"README.md\",\n            \"README.txt\",\n        ]\n        self.ignore_extensions = [\n            \"whitesource\",\n            \"gitignore\",\n            \"png\",\n            \"jpg\",\n            \"woff2\",\n            \"PNG\",\n            \"woff\",\n            \"gif\",\n            \"csv\",\n            \"xlsx\",\n            \"xls\",\n        ]\n\n    def get_token(self) -&gt; str:\n        \"\"\"\n        Get the GitHub token for the GitHub App. This harnesses the GitHub App installation token.\n\n        Parameters:\n            None\n\n        Returns:\n            github_token['token'](str): GitHub token for the GitHub App.\n        \"\"\"\n        app_id = 340993\n        installation_id = 38193318\n\n        with open(self.pem_file, \"w\") as pem_file_open:\n            pem_file_open.write(self.pem_key)\n\n        with open(self.pem_file, \"rb\") as pem_file_open:\n            signing_key = jwt.jwk_from_pem(pem_file_open.read())\n\n        payload = {\n            \"iat\": int(time.time()),\n            \"exp\": int(time.time()) + 600,\n            \"iss\": app_id,\n        }\n\n        jwt_instance = jwt.JWT()\n        encoded_jwt = jwt_instance.encode(payload, signing_key, \"RS256\")\n\n        url = (\n            f\"https://api.github.com/app/installations/{installation_id}/access_tokens\"\n        )\n        headers = {\n            \"Accept\": \"application/vnd.github+json\",\n            \"Authorization\": f\"Bearer {encoded_jwt}\",\n            \"X-GitHub-Api-Version\": \"2022-11-28\",\n        }\n\n        github_token = requests.post(url, headers=headers).json()\n        return github_token[\"token\"]\n\n    def num_tokens_from_string(self, string: str, encoding_name: str) -&gt; int:\n        \"\"\"\n        Get the number of tokens from a string based on the encoding.\n\n        Parameters:\n            string (str): The string to be tokenized.\n            encoding_name (str): The encoding to be used for tokenization.\n\n        Returns:\n            len(encoding.encode(string))(int): Number of tokens in the string.\n        \"\"\"\n        encoding = tiktoken.get_encoding(encoding_name)\n        return len(encoding.encode(string))\n\n    @staticmethod\n    def nlp_process(html):\n        \"\"\"\n        Process the HTML content to remove unwanted characters.\n\n        Parameters:\n            html (str): HTML content to be processed.\n\n        Returns:\n            text(str): Processed text content.\n        \"\"\"\n        try:\n            soup = BeautifulSoup(html, \"html.parser\")\n            text = soup.get_text()\n            text = re.sub(r\"\\[[^\\]]*\\]\", \"\", text)\n            text = text.replace(\"\\n\", \" \")\n            text = text.replace(\"\\n\\n\", \" \")\n            text = re.sub(r\"[_{}*#\\[\\]]\", \"\", text)\n            text = re.sub(r\"\\s+\", \" \", text).strip()\n            return text\n        except Exception:\n            return \"Cannot be decoded\"\n\n    @staticmethod\n    def get_directory_level(file_path):\n        return file_path.count(\"/\")\n\n    @staticmethod\n    def calculate_token_count(text):\n        word_count = len(text.split())\n        return word_count * 4 / 3\n\n    def get_new_github_instance(self, token):\n        return Github(token)\n\n    def get_organization_repos_data(self, g, start_time):\n        \"\"\"\n        Get the data from the repositories in the organization.\n\n        Parameters:\n            g (Github): GitHub instance\n            start_time (float): Start time of the process.\n\n        Returns:\n            repo_contents_df (DataFrame): Dataframe containing the repository contents.\n\n        \"\"\"\n        files = []\n        print(\"\\nBeginning data collection process...\")\n        for repo_item in self.repo_list:\n            if time.time() &gt; start_time + 3540:\n                sleep_time = 65\n                print(\n                    f\"Putting to sleep for {sleep_time}, reason: near 60 min runtime.\"\n                )\n                time.sleep(sleep_time)\n                start_time = time.time()\n                g = self.get_new_github_instance(self.get_token())\n\n            try:\n                repo = g.get_organization(self.organization_name).get_repo(repo_item)\n                contents = repo.get_contents(\"\")\n                num_token_collected = 0\n                while contents and num_token_collected &lt; self.num_token_per_repo:\n                    file_content = contents.pop(0)\n                    if file_content.type == \"dir\":\n                        contents.extend(repo.get_contents(file_content.path))\n                    else:\n                        file_name = file_content.path.split(\"/\")[-1]\n                        if file_name not in self.ignore_files:\n                            file_extension = file_name.split(\".\")[-1]\n                            if file_extension not in self.ignore_extensions:\n                                try:\n                                    if file_extension == \"sql\":\n                                        decoded_content = file_content.decoded_content\n                                    else:\n                                        decoded_content = (\n                                            file_content.decoded_content.decode(\"utf-8\")\n                                        )\n\n                                    cleaned_content = self.nlp_process(decoded_content)\n                                    token_count = self.num_tokens_from_string(\n                                        cleaned_content, \"cl100k_base\"\n                                    )\n                                    directory_level = self.get_directory_level(\n                                        file_content.path\n                                    )\n                                    if (\n                                        num_token_collected + token_count\n                                        &lt;= self.num_token_per_repo\n                                    ):\n                                        num_token_collected += token_count\n                                        files.append(\n                                            (\n                                                repo_item,\n                                                file_content.name,\n                                                file_content.path,\n                                                cleaned_content,\n                                                token_count,\n                                                num_token_collected,\n                                                directory_level,\n                                            )\n                                        )\n                                except Exception as e:\n                                    print(\n                                        f\"Error decoding file from repository {repo_item}: {e}\"\n                                    )\n                                    print(\"Problem file: \", file_content.path)\n            except Exception as e:\n                print(f\"Error for repository {repo_item}: {e}\")\n\n        repo_contents_df = pd.DataFrame(\n            files,\n            columns=[\n                \"RepoName\",\n                \"FileName\",\n                \"FilePath\",\n                \"DecodedContent\",\n                \"TokenCountPerFile\",\n                \"CumulativeRepoContent\",\n                \"DirectoryLevel\",\n            ],\n        )\n\n        return repo_contents_df\n\n    def data_collection(self):\n        \"\"\"\n        Collect repostiory contents from the repositories in the organization and return as a dataframe.\n\n        Parameters:\n            None\n\n        Returns:\n            repo_contents_df (DataFrame): Dataframe containing the repository contents.\n\n        \"\"\"\n        token = self.get_token()\n        start_time = time.time()\n        g = self.get_new_github_instance(token)\n\n        repo_contents_df = self.get_organization_repos_data(g, start_time)\n        return repo_contents_df\n\n    def concatenate_repo_contents(self, repo_contents_df):\n        \"\"\"\n        Concatenate the contents of the repositories into a dataframe.\n\n        Parameters:\n            repo_contents_df (DataFrame): Dataframe containing the repository contents.\n\n        Returns:\n            result_df (DataFrame): Dataframe containing the concatenated contents of the repositories.\n        \"\"\"\n        result = []\n        grouped = repo_contents_df.groupby(\"RepoName\")\n\n        for repo_name, group in grouped:\n            group = group[group[\"DecodedContent\"].str.strip() != \"\"]\n            concatenated_content = \", \".join(group[\"DecodedContent\"])\n            total_token_count = group[\"TokenCountPerFile\"].sum()\n            files_included = \", \".join(group[\"FileName\"])\n            result.append(\n                (repo_name, concatenated_content, total_token_count, files_included)\n            )\n\n        result_df = pd.DataFrame(\n            result,\n            columns=[\n                \"RepoName\",\n                \"ConcatenatedContent\",\n                \"TotalTokenCount\",\n                \"FilesIncluded\",\n            ],\n        )\n        return result_df\n</code></pre>"},{"location":"code-reference/RepoContentsTokenisation/#src.fleming.code_summary.repo_contents_tokenisation.GitHubRepoDataProcessor.get_token","title":"<code>get_token()</code>","text":"<p>Get the GitHub token for the GitHub App. This harnesses the GitHub App installation token.</p> <p>Returns:</p> Type Description <code>str</code> <p>github_token'token': GitHub token for the GitHub App.</p> Source code in <code>src/fleming/code_summary/repo_contents_tokenisation.py</code> <pre><code>def get_token(self) -&gt; str:\n    \"\"\"\n    Get the GitHub token for the GitHub App. This harnesses the GitHub App installation token.\n\n    Parameters:\n        None\n\n    Returns:\n        github_token['token'](str): GitHub token for the GitHub App.\n    \"\"\"\n    app_id = 340993\n    installation_id = 38193318\n\n    with open(self.pem_file, \"w\") as pem_file_open:\n        pem_file_open.write(self.pem_key)\n\n    with open(self.pem_file, \"rb\") as pem_file_open:\n        signing_key = jwt.jwk_from_pem(pem_file_open.read())\n\n    payload = {\n        \"iat\": int(time.time()),\n        \"exp\": int(time.time()) + 600,\n        \"iss\": app_id,\n    }\n\n    jwt_instance = jwt.JWT()\n    encoded_jwt = jwt_instance.encode(payload, signing_key, \"RS256\")\n\n    url = (\n        f\"https://api.github.com/app/installations/{installation_id}/access_tokens\"\n    )\n    headers = {\n        \"Accept\": \"application/vnd.github+json\",\n        \"Authorization\": f\"Bearer {encoded_jwt}\",\n        \"X-GitHub-Api-Version\": \"2022-11-28\",\n    }\n\n    github_token = requests.post(url, headers=headers).json()\n    return github_token[\"token\"]\n</code></pre>"},{"location":"code-reference/RepoContentsTokenisation/#src.fleming.code_summary.repo_contents_tokenisation.GitHubRepoDataProcessor.num_tokens_from_string","title":"<code>num_tokens_from_string(string, encoding_name)</code>","text":"<p>Get the number of tokens from a string based on the encoding.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>The string to be tokenized.</p> required <code>encoding_name</code> <code>str</code> <p>The encoding to be used for tokenization.</p> required <p>Returns:</p> Name Type Description <code>len</code> <code>encoding.encode(string))(int</code> <p>Number of tokens in the string.</p> Source code in <code>src/fleming/code_summary/repo_contents_tokenisation.py</code> <pre><code>def num_tokens_from_string(self, string: str, encoding_name: str) -&gt; int:\n    \"\"\"\n    Get the number of tokens from a string based on the encoding.\n\n    Parameters:\n        string (str): The string to be tokenized.\n        encoding_name (str): The encoding to be used for tokenization.\n\n    Returns:\n        len(encoding.encode(string))(int): Number of tokens in the string.\n    \"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    return len(encoding.encode(string))\n</code></pre>"},{"location":"code-reference/RepoContentsTokenisation/#src.fleming.code_summary.repo_contents_tokenisation.GitHubRepoDataProcessor.nlp_process","title":"<code>nlp_process(html)</code>  <code>staticmethod</code>","text":"<p>Process the HTML content to remove unwanted characters.</p> <p>Parameters:</p> Name Type Description Default <code>html</code> <code>str</code> <p>HTML content to be processed.</p> required <p>Returns:</p> Name Type Description <code>text</code> <code>str</code> <p>Processed text content.</p> Source code in <code>src/fleming/code_summary/repo_contents_tokenisation.py</code> <pre><code>@staticmethod\ndef nlp_process(html):\n    \"\"\"\n    Process the HTML content to remove unwanted characters.\n\n    Parameters:\n        html (str): HTML content to be processed.\n\n    Returns:\n        text(str): Processed text content.\n    \"\"\"\n    try:\n        soup = BeautifulSoup(html, \"html.parser\")\n        text = soup.get_text()\n        text = re.sub(r\"\\[[^\\]]*\\]\", \"\", text)\n        text = text.replace(\"\\n\", \" \")\n        text = text.replace(\"\\n\\n\", \" \")\n        text = re.sub(r\"[_{}*#\\[\\]]\", \"\", text)\n        text = re.sub(r\"\\s+\", \" \", text).strip()\n        return text\n    except Exception:\n        return \"Cannot be decoded\"\n</code></pre>"},{"location":"code-reference/RepoContentsTokenisation/#src.fleming.code_summary.repo_contents_tokenisation.GitHubRepoDataProcessor.get_organization_repos_data","title":"<code>get_organization_repos_data(g, start_time)</code>","text":"<p>Get the data from the repositories in the organization.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>Github</code> <p>GitHub instance</p> required <code>start_time</code> <code>float</code> <p>Start time of the process.</p> required <p>Returns:</p> Name Type Description <code>repo_contents_df</code> <code>DataFrame</code> <p>Dataframe containing the repository contents.</p> Source code in <code>src/fleming/code_summary/repo_contents_tokenisation.py</code> <pre><code>def get_organization_repos_data(self, g, start_time):\n    \"\"\"\n    Get the data from the repositories in the organization.\n\n    Parameters:\n        g (Github): GitHub instance\n        start_time (float): Start time of the process.\n\n    Returns:\n        repo_contents_df (DataFrame): Dataframe containing the repository contents.\n\n    \"\"\"\n    files = []\n    print(\"\\nBeginning data collection process...\")\n    for repo_item in self.repo_list:\n        if time.time() &gt; start_time + 3540:\n            sleep_time = 65\n            print(\n                f\"Putting to sleep for {sleep_time}, reason: near 60 min runtime.\"\n            )\n            time.sleep(sleep_time)\n            start_time = time.time()\n            g = self.get_new_github_instance(self.get_token())\n\n        try:\n            repo = g.get_organization(self.organization_name).get_repo(repo_item)\n            contents = repo.get_contents(\"\")\n            num_token_collected = 0\n            while contents and num_token_collected &lt; self.num_token_per_repo:\n                file_content = contents.pop(0)\n                if file_content.type == \"dir\":\n                    contents.extend(repo.get_contents(file_content.path))\n                else:\n                    file_name = file_content.path.split(\"/\")[-1]\n                    if file_name not in self.ignore_files:\n                        file_extension = file_name.split(\".\")[-1]\n                        if file_extension not in self.ignore_extensions:\n                            try:\n                                if file_extension == \"sql\":\n                                    decoded_content = file_content.decoded_content\n                                else:\n                                    decoded_content = (\n                                        file_content.decoded_content.decode(\"utf-8\")\n                                    )\n\n                                cleaned_content = self.nlp_process(decoded_content)\n                                token_count = self.num_tokens_from_string(\n                                    cleaned_content, \"cl100k_base\"\n                                )\n                                directory_level = self.get_directory_level(\n                                    file_content.path\n                                )\n                                if (\n                                    num_token_collected + token_count\n                                    &lt;= self.num_token_per_repo\n                                ):\n                                    num_token_collected += token_count\n                                    files.append(\n                                        (\n                                            repo_item,\n                                            file_content.name,\n                                            file_content.path,\n                                            cleaned_content,\n                                            token_count,\n                                            num_token_collected,\n                                            directory_level,\n                                        )\n                                    )\n                            except Exception as e:\n                                print(\n                                    f\"Error decoding file from repository {repo_item}: {e}\"\n                                )\n                                print(\"Problem file: \", file_content.path)\n        except Exception as e:\n            print(f\"Error for repository {repo_item}: {e}\")\n\n    repo_contents_df = pd.DataFrame(\n        files,\n        columns=[\n            \"RepoName\",\n            \"FileName\",\n            \"FilePath\",\n            \"DecodedContent\",\n            \"TokenCountPerFile\",\n            \"CumulativeRepoContent\",\n            \"DirectoryLevel\",\n        ],\n    )\n\n    return repo_contents_df\n</code></pre>"},{"location":"code-reference/RepoContentsTokenisation/#src.fleming.code_summary.repo_contents_tokenisation.GitHubRepoDataProcessor.data_collection","title":"<code>data_collection()</code>","text":"<p>Collect repostiory contents from the repositories in the organization and return as a dataframe.</p> <p>Returns:</p> Name Type Description <code>repo_contents_df</code> <code>DataFrame</code> <p>Dataframe containing the repository contents.</p> Source code in <code>src/fleming/code_summary/repo_contents_tokenisation.py</code> <pre><code>def data_collection(self):\n    \"\"\"\n    Collect repostiory contents from the repositories in the organization and return as a dataframe.\n\n    Parameters:\n        None\n\n    Returns:\n        repo_contents_df (DataFrame): Dataframe containing the repository contents.\n\n    \"\"\"\n    token = self.get_token()\n    start_time = time.time()\n    g = self.get_new_github_instance(token)\n\n    repo_contents_df = self.get_organization_repos_data(g, start_time)\n    return repo_contents_df\n</code></pre>"},{"location":"code-reference/RepoContentsTokenisation/#src.fleming.code_summary.repo_contents_tokenisation.GitHubRepoDataProcessor.concatenate_repo_contents","title":"<code>concatenate_repo_contents(repo_contents_df)</code>","text":"<p>Concatenate the contents of the repositories into a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>repo_contents_df</code> <code>DataFrame</code> <p>Dataframe containing the repository contents.</p> required <p>Returns:</p> Name Type Description <code>result_df</code> <code>DataFrame</code> <p>Dataframe containing the concatenated contents of the repositories.</p> Source code in <code>src/fleming/code_summary/repo_contents_tokenisation.py</code> <pre><code>def concatenate_repo_contents(self, repo_contents_df):\n    \"\"\"\n    Concatenate the contents of the repositories into a dataframe.\n\n    Parameters:\n        repo_contents_df (DataFrame): Dataframe containing the repository contents.\n\n    Returns:\n        result_df (DataFrame): Dataframe containing the concatenated contents of the repositories.\n    \"\"\"\n    result = []\n    grouped = repo_contents_df.groupby(\"RepoName\")\n\n    for repo_name, group in grouped:\n        group = group[group[\"DecodedContent\"].str.strip() != \"\"]\n        concatenated_content = \", \".join(group[\"DecodedContent\"])\n        total_token_count = group[\"TokenCountPerFile\"].sum()\n        files_included = \", \".join(group[\"FileName\"])\n        result.append(\n            (repo_name, concatenated_content, total_token_count, files_included)\n        )\n\n    result_df = pd.DataFrame(\n        result,\n        columns=[\n            \"RepoName\",\n            \"ConcatenatedContent\",\n            \"TotalTokenCount\",\n            \"FilesIncluded\",\n        ],\n    )\n    return result_df\n</code></pre>"},{"location":"contact/email/","title":"Contact via Email","text":"<p>If you need to contact us directly, we're here to help! You can reach out to us through our official email. Whether you have questions, feedback, or need support, our team is ready to assist you. We value your input and strive to respond promptly to all inquiries. </p> <p>Your communication helps us improve and provide better services, so don't hesitate to get in touch!</p> <p>Emails:</p> <ul> <li>christian.defeo@shell.com</li> </ul>"},{"location":"contact/github/","title":"Reach out on Github","text":"<p>You can interact with us directly on GitHub by raising issues and creating pull requests (PRs). </p> <p>If you encounter any bugs or have suggestions for improvements, simply open an issue to let us know. </p> <p>For those who want to contribute code, you can fork our repository, make your changes, and submit a pull request. We review all contributions and appreciate the community's efforts to enhance our project. Your involvement helps us grow and improve, and we look forward to seeing your contributions!</p> <p>For more on how to contirbute please follow our Contributing guide.</p>"},{"location":"getting-started/calling-the-endpoint/","title":"Query the Endpoint","text":""},{"location":"getting-started/calling-the-endpoint/#documentation","title":"Documentation","text":"<p>Once you have your model serving on a databricks endpoint it is then possible to query this data.</p> <p>Please find an example below.</p> <p>For more information about options within the Class please follow the documentation under the code-reference section.</p>"},{"location":"getting-started/calling-the-endpoint/#example","title":"Example","text":"<pre><code>url = \"https://example.com/model_endpoint\"\ntoken = \"your_auth_token\"\n\n# Create an instance of ModelQuery\nmodel_query = ModelQuery(url, token)\n\n# Example dataset\ndataset = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6]})\n\ntry:\n    # Score the model using the dataset\n    response = model_query.score_model(dataset)\n    print(response)\nexcept requests.exceptions.HTTPError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre>"},{"location":"getting-started/code-summary-how-to/","title":"Code Summarisation Using LLM for Search Index Enhancement","text":""},{"location":"getting-started/code-summary-how-to/#documentation","title":"Documentation","text":"<p>Once you have extracted and concatenated the contents of a repository, this class can be used to then generate descriptive documentation using Azure OpenAI. Please note, this class requires you to have a working Azure OpenAI API Key. </p> <p>This class then authenticates to the API, and passes in the concatenated repository content with a prompt of your choosing (an example has been provided below). The output is descriptive documentation that identifies code functionality, which can then be indexed by Fleming, to allow users to find specific code functionality that may not have been sufficiently detailed in the readme, or hidden by a domain context.</p> <p>For more information about options within the Class please follow the documentation under the code-reference section.</p>"},{"location":"getting-started/code-summary-how-to/#example","title":"Example","text":"<pre><code>```python\nfrom fleming.code_summary.fourO_mini_summary import call_openai\nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"openai_client\").getOrCreate()\n\nspark_input_df = \"your_spark_input_df\"\noutput_table_name = \"your_output_table\"\n\nprompt = \"The following code is the contents of a repository, generate a short summary paragraph describing what the repository purpose is. A paragraph detailing the key functionalities and technologies integrate with and a list of key words associated with this repository underneath. Focus on the purpose of the code contained in the repository, and the technologies, data and platforms it integrates with\"\n\napi_key = \"your_api_key\"\nendpoint = \"https://api.openai.com/yourendpointhere\"\n\nheaders = {\n\"Content-Type\": \"application/json\",\n\"api-key\": api_key,\n}\n\nclient = OpenAIClient(spark, delta_table, output_table_name, prompt, api_key, endpoint, headers)\nclient.call_openai()\n```\n</code></pre>"},{"location":"getting-started/creating-the-corpus/","title":"Create the Corpus","text":""},{"location":"getting-started/creating-the-corpus/#documentation","title":"Documentation","text":"<p>The first step is to create a corpus.txt file, this includes all the valuse you will search over. The below function ingests a dataframe each row being a seperate entry into the corpus. </p> <p>Please find an example below.</p> <p>For more information about options within the Class please follow the documentation under the code-reference section.</p>"},{"location":"getting-started/creating-the-corpus/#example","title":"Example","text":"<pre><code>from fleming.discovery.corpus_creation import CorpusCreation\nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"corpus_creation\").getOrCreate()\n\ncorpus_df = spark.read.csv(\"/tmp/corpus.csv\", header=True, inferSchema=True)\ncorpus_file_path = \"/tmp/search_corpus.txt\"\n\ncorpus_creation = CorpusCreation(corpus_df, corpus_file_path)\ncorpus = corpus_creation.concat_columns(df_analytics_cleaned)\ncorpus_creation.write_corpus_to_file(corpus)\n</code></pre>"},{"location":"getting-started/creating-the-model/","title":"Create and Register the Model","text":""},{"location":"getting-started/creating-the-model/#documentation","title":"Documentation","text":"<p>After the corpus.txt file has been created it is now possible to load the corpus to an open source semantic search model and register the model with databricks. Project Fleming embraces open-source and can be used with any open-source model on Hugging Face.</p> <p>Please find an example below.</p> <p>For more information about options within the Class please follow the documentation under the code-reference section.</p>"},{"location":"getting-started/creating-the-model/#example","title":"Example","text":"<pre><code>from fleming.discovery.model_train_register import ModelTrainRegister, SemanticSearchModel    \nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"model_serving\").getOrCreate()\n\nmodel_directory = \"/tmp/BERT_Semantic_Search_model\"\ncorpus_file = \"/tmp/search_corpus.txt\"\ncorpus_embedding_file = '/tmp/corpus_embedding.pt'\n\nmodel_developer = ModelTrainRegister(spark, model_directory, corpus_file, corpus_embedding_file)\n\n# Register the model\nsemantic_search_model = \"multi-qa-mpnet-base-dot-v1\"\nmodel_developer.register_model(semantic_search_model)\n\n# Embed the corpus\nmodel_developer.embed_corpus()\n\n# Define parameters and artifacts\nparameters = {\"top_k\": 50, \"relevancy_score\": 0.45}\ninput_example = [\"Innersource best practices\"]\ntest_output = [\"match 1\", \"match 2\"]\nsignature = infer_signature(input_example, test_output, params=parameters)\nartifacts = {\n    \"model_path\": model_directory,\n    \"corpus_file\": corpus_file,\n    \"corpus_embedding_file\": corpus_embedding_file\n}\nunique_model_name = \"semantic_search_model\"\n\n# Create and serve the model\nexperiment_location = \"/path/to/experiment\"\nmodel_developer.create_registered_model(unique_model_name, input_example, signature, artifacts, experiment_location)\n</code></pre>"},{"location":"getting-started/deploying-the-frontend/","title":"How-To Guide: Generic Frontend for Project Fleming","text":"<p>(Guide current as of: Monday, April 14, 2025)</p>"},{"location":"getting-started/deploying-the-frontend/#introduction","title":"Introduction","text":"<p>This guide will walk you through: 1.  Prerequisites 2.  Setting up the development environment 3.  Configuring the application to connect to your Project Fleming ML API 4.  Running the application locally 5.  Understanding the basic architecture</p>"},{"location":"getting-started/deploying-the-frontend/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed and available:</p> <ol> <li>Node.js: (Version 20.x or later recommended for Next.js). You can download it from nodejs.org.</li> <li>Package Manager: <code>npm</code> (comes with Node.js), <code>yarn</code>, <code>pnpm</code>, or <code>bun</code>. Choose one you are comfortable with.</li> <li>Git: For cloning the repository.</li> <li>Project Fleming ML API Endpoint: You need the URL where your instance of the Project Fleming ML backend API is running.</li> <li>API Authentication Token: You need the authentication token required to securely access the Project Fleming ML API.</li> </ol> <p>(You should obtain the ML API endpoint and Auth Token from the team or documentation related to your specific Project Fleming backend deployment.)</p>"},{"location":"getting-started/deploying-the-frontend/#getting-started-setup-and-running","title":"Getting Started: Setup and Running","text":"<p>Follow these steps to get the frontend running on your local machine:</p> <ol> <li> <p>Clone the Repository:     Open your terminal or command prompt and run:     <pre><code>git clone &lt;repository-url&gt; # Replace &lt;repository-url&gt; with the actual URL of your frontend's Git repository\ncd &lt;repository-directory&gt;   # Navigate into the cloned project folder\n</code></pre></p> </li> <li> <p>Install Dependencies:     Using your preferred package manager, install the necessary project dependencies:     <pre><code># Using npm\nnpm install\n\n# Or using yarn\nyarn install\n\n# Or using pnpm\npnpm install\n\n# Or using bun\nbun install\n</code></pre></p> </li> <li> <p>Configure Environment Variables:     This application needs to know where the Project Fleming ML API is located and how to authenticate with it.</p> <ul> <li>Create a new file named <code>.env</code> in the root directory of the project.</li> <li>Open the <code>.env</code> file and add the following lines, replacing the placeholder values with your actual API details:</li> </ul> <p><pre><code># .env file content\n\n# The full URL to your Project Fleming ML API endpoint\nML_API=https://your-project-fleming-api.example.com/api/search\n\n# The authentication token required by the API\nAPI_AUTH_TOKEN=your_secret_auth_token_here\n</code></pre> * Important: Ensure the <code>ML_API</code> value is the correct endpoint provided by your Project Fleming backend for making search/prediction requests. Save the <code>.env</code> file. This file is typically ignored by Git (via <code>.gitignore</code>) to keep your secrets safe.</p> </li> <li> <p>Run the Development Server:     Start the Next.js development server:     <pre><code># Using npm\nnpm run dev\n\n# Or using yarn\nyarn dev\n\n# Or using pnpm\npnpm dev\n\n# Or using bun\nbun dev\n</code></pre></p> </li> <li> <p>Access the Application:     Open your web browser and navigate to:     <code>http://localhost:3000</code></p> <p>You should now see the generic frontend application running.</p> </li> </ol>"},{"location":"getting-started/deploying-the-frontend/#how-it-works-architecture-overview","title":"How It Works: Architecture Overview","text":"<p>Understanding the flow of data is key to using and potentially customizing this frontend:</p> <ol> <li>User Interaction: The user interacts with the UI, likely entering a search query into an input field in the page located at <code>app/page.tsx</code>.</li> <li>Frontend API Call: When the user submits their query, the frontend code (in <code>app/page.tsx</code> or related components) makes a request to its own internal API route: <code>/api/predictions</code>.</li> <li>Internal API Route (<code>/api/predictions</code>):<ul> <li>This Next.js API route (defined in <code>app/api/predictions/route.ts</code>) acts as a proxy or intermediary.</li> <li>It receives the request from the frontend UI.</li> <li>It reads the <code>ML_API</code> and <code>API_AUTH_TOKEN</code> from the environment variables (which you set in the <code>.env</code> file).</li> <li>It then makes a secure request (using the auth token) to the actual Project Fleming ML API (the URL specified in <code>ML_API</code>).</li> <li>It receives the raw prediction data (e.g., list of repos, articles) from the ML API.</li> <li>It perform some basic processing or reformatting on this data to make it suitable for the frontend UI.</li> <li>It sends the processed data back as a response to the frontend UI component that initially called it.</li> </ul> </li> <li>Displaying Results: The frontend UI component (<code>app/page.tsx</code>) receives the processed data from <code>/api/predictions</code> and renders the results (e.g., a list of findings) for the user to see.</li> </ol> <p>This architecture keeps your ML API endpoint and token secure on the server-side (within the <code>/api/predictions</code> route) rather than exposing them directly in the browser.</p>"},{"location":"getting-started/deploying-the-frontend/#using-the-frontend","title":"Using the Frontend","text":"<p>Once running, you can typically:</p> <ol> <li>Find the search input field on the page (<code>http://localhost:3000</code>).</li> <li>Type in your query related to the kind of information Project Fleming indexes (e.g., \"how to implement authentication in python\", \"database connection pooling best practices\", \"find repository for UI components\").</li> <li>Submit the query (e.g., by pressing Enter or clicking a search button).</li> <li>View the results displayed on the page, which are fetched from your configured Project Fleming ML API via the frontend's internal API.</li> </ol>"},{"location":"getting-started/deploying-the-frontend/#customization","title":"Customization","text":"<p>If you want to modify the frontend:</p> <ul> <li>UI Changes: Edit the React component file located at <code>app/page.tsx</code>. This is where the main page structure, input fields, and results display logic reside. The page will auto-update in your browser as you save changes when the development server is running.</li> <li>Data Handling/Processing: If you need to change how data from the ML API is processed before being shown in the UI, modify the internal API route handler (<code>app/api/predictions/route.ts</code> or a similar file depending on your specific setup).</li> </ul>"},{"location":"getting-started/deploying-the-frontend/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Error connecting to API / No results:<ul> <li>Double-check the <code>ML_API</code> URL in your <code>.env</code> file. Is it correct and accessible? Is the path correct?</li> <li>Verify the <code>API_AUTH_TOKEN</code> in your <code>.env</code> file. Is it valid?</li> <li>Ensure the Project Fleming backend server is running and reachable from where you are running the frontend development server.</li> <li>Check the console output in your terminal (where you ran <code>npm run dev</code>) and the browser's developer console (F12) for specific error messages. Errors might originate from the <code>/api/predictions</code> route or the Project Fleming backend itself.</li> </ul> </li> <li>Application doesn't start:<ul> <li>Ensure all dependencies were installed correctly (<code>npm install</code> or equivalent).</li> <li>Make sure you have a compatible version of Node.js installed.</li> <li>Check if the <code>.env</code> file exists, even if you haven't filled it yet, as some setups might expect it.</li> </ul> </li> </ul>"},{"location":"getting-started/deploying-the-frontend/#advanced-settings","title":"Advanced Settings","text":"<p>The application includes an Advanced Settings feature that allows users to override the default ML API configuration without modifying environment variables. This is particularly useful for:</p> <ul> <li>Testing against different ML API endpoints</li> <li>Using custom authentication tokens</li> <li>Development and debugging scenarios</li> <li>Multi-environment configurations</li> </ul>"},{"location":"getting-started/deploying-the-frontend/#features","title":"Features","text":"<ul> <li>Custom API URL: Override the default <code>ML_API</code> endpoint with a custom URL</li> <li>Custom Auth Token: Override the default <code>API_AUTH_TOKEN</code> with a custom authentication token</li> <li>Persistent Storage: Settings are saved in browser's localStorage and persist across sessions</li> <li>Visual Indicators: The Advanced Settings button shows a blue indicator when custom settings are active</li> <li>Reset Functionality: Easily reset to default environment variable values</li> </ul>"},{"location":"getting-started/deploying-the-frontend/#how-to-use-advanced-settings","title":"How to Use Advanced Settings","text":"<ol> <li>Access: Click the \"Advanced\" button in the search interface</li> <li>Configure: Enter your custom API URL and/or authentication token in the dialog</li> <li>Save: Click \"Save Settings\" to apply and persist your changes</li> <li>Reset: Use \"Reset to Default\" to clear custom settings and return to environment variables</li> </ol>"},{"location":"getting-started/deploying-the-frontend/#settings-priority","title":"Settings Priority","text":"<p>The application follows this priority order for configuration:</p> <ol> <li>Custom Settings (highest priority) - Settings configured through the Advanced Settings dialog</li> <li>Environment Variables (fallback) - Default settings from <code>.env</code> file</li> </ol> <p>When custom settings are active, they completely override the corresponding environment variables for that session.</p>"},{"location":"getting-started/deploying-the-frontend/#storage","title":"Storage","text":"<ul> <li>Custom settings are stored in the browser's localStorage</li> <li>Settings persist across browser sessions until manually reset</li> <li>Settings are scoped to the specific browser/device</li> </ul>"},{"location":"getting-started/deploying-the-frontend/#api","title":"API","text":""},{"location":"getting-started/deploying-the-frontend/#apipredictions","title":"<code>api/predictions</code>","text":"<p>This API endpoint is responsible for fetching an array of predictions from the machine learning model. It processes the data to ensure it is easy for the frontend to render.</p> <p>To use this endpoint, make a GET request to <code>/api/predictions</code>. The response will include the processed predictions ready for display on the frontend.</p> <p>The endpoint automatically uses either the environment variables or custom settings (if configured) for ML API communication.</p>"},{"location":"getting-started/deploying-the-frontend/#learn-more","title":"Learn More","text":"<p>To learn more about Next.js, take a look at the following resources:</p> <ul> <li>Next.js Documentation - learn about Next.js features and API.</li> <li>Learn Next.js - an interactive Next.js tutorial.</li> </ul> <p>You can check out the Next.js GitHub repository - your feedback and contributions are welcome!</p>"},{"location":"getting-started/installation/","title":"Installation Process","text":""},{"location":"getting-started/installation/#prerequisite","title":"Prerequisite","text":""},{"location":"getting-started/installation/#python","title":"Python","text":"<p>There are a few things to note before using the Fleming. The following prerequisites will need to be installed on your local machine.</p> <p>Python version 3.9 &gt;= and &lt; 3.12 should be installed. Check which python version you have with the following command:</p> <pre><code>python --version\n</code></pre> <p>Find the latest python version here and ensure your python path is set up correctly on your machine.</p>"},{"location":"getting-started/installation/#python-package-installers","title":"Python Package Installers","text":"<p>Installing Fleming can be done using the package installer Micromamba.</p>"},{"location":"getting-started/installation/#java","title":"Java","text":"<p>To use Fleming in your own environment that leverages pyspark, Java 8 or later is a prerequisite. See below for suggestions to install Java in your development environment.</p> <p>Follow the official Java JDK installation documentation here.</p> <ul> <li>Windows</li> <li>Mac OS</li> <li>Linux</li> </ul> <p>Note</p> <p>Windows requires an additional installation of a file called winutils.exe. Please see this repo for more information.</p>"},{"location":"getting-started/installation/#installation","title":"Installation","text":"<p>1) To get started with developing for this project, clone the repository.  <pre><code>    git clone https://github.com/sede-open/Fleming.git\n</code></pre> 2) Open the respository in VS Code, Visual Studio or your preferered code editor.</p> <p>3) Create a new environment using the following command: <pre><code>    micromamba create -f environment.yml\n</code></pre></p> <p>NOTE:  You will need to have conda, python and pip installed to use the command above.</p> <p>4) Activate your newly set up environment using the following command: <pre><code>    micromamba activate fleming\n</code></pre> You are now ready to start developing your own functions. Please remember to follow Felming's development lifecycle to maintain clarity and efficiency for a fully robust self serving platform. </p> <p>5) For better readability of code is would be useful to enable black and isort on autosave by simply adding this to the VSCode user settings json(Ctrl + Shft + P):</p> <pre><code>    {\n        \"editor.formatOnSave\": true,\n        \"python.formatting.provider\": \"black\",\n        \"python.formatting.blackArgs\": [\n            \"--line-length=119\"\n        ],\n        \"python.sortImports.args\": [\n            \"--profile\",\n            \"black\"\n        ],\n        \"[python]\": {\n            \"editor.codeActionsOnSave\": {\n                \"source.organizeImports\": true\n            }\n        }\n    }\n</code></pre>"},{"location":"getting-started/serving-the-model/","title":"Serve the Registered Model","text":""},{"location":"getting-started/serving-the-model/#documentation","title":"Documentation","text":"<p>After the model has been registered it is now possible to serve the model with the databricks serving endpoint. A unique part of Fleming is that models created can be run on Small CPU Clusters which is both cost and energy efficient.</p> <p>Please find an example below.</p> <p>For more information about options within the Class please follow the documentation under the code-reference section.</p> <pre><code>from fleming.discovery.model_serve import ModelServewithMosaicAI, ModelServe\nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"model_serving\").getOrCreate()\n\n# Set the name of the MLflow endpoint\nendpoint_name = \"aidiscoverytool\"\nprint(f'Endpoint name: {endpoint_name}')\n\n# Name of the registered MLflow model\nmodel_name = \"BERT_Semantic_Search\" \nprint(f'Model name: {model_name}')\n\n# Get the latest version of the MLflow model\nmodel_version = MlflowClient().get_registered_model(model_name).latest_versions[1].version \nprint(f'Model version: {model_version}')\n\n# Specify the type of compute (CPU, GPU_SMALL, GPU_LARGE, etc.)\nworkload_type = \"CPU\"\nprint(f'Workload type: {workload_type}')\n\n# Specify the scale-out size of compute (Small, Medium, Large, etc.)\nworkload_size = \"Small\"\nprint(f'Workload size: {workload_size}')\n\n# Specify Scale to Zero(only supported for CPU endpoints)\nscale_to_zero = False\nprint(f'Scale to zero: {scale_to_zero}')\n\nAPI_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\nAPI_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n\n# You can now choose how you serve your model, with or without Mosaic AI\n\nmodel_serve = ModelServe(endpoint_name, model_name, workload_type, workload_size, scale_to_zero, API_ROOT, API_TOKEN)\nmodel_serve.deploy_endpoint()\n\nmodel_serve = ModelServewithMosaicAI(endpoint_name, model_name, workload_type, workload_size, scale_to_zero, API_ROOT, API_TOKEN)\nmodel_serve.deploy_endpoint()\n</code></pre>"},{"location":"getting-started/tokenisation-of-git-repo/","title":"Extracting GitHub Repo Contents and Concatenating for Summarisation","text":""},{"location":"getting-started/tokenisation-of-git-repo/#documentation","title":"Documentation","text":"<p>To prepare a github repository to be summarised it is important to extract all relevant files and concantenate the information. This class ingests the files from a list of repositories in an organization and processes the contents to return a dataframe of the contents concatenated.</p> <p>Different summerisation tools will have different limits on the number of tokens which can be ingested, the below class allows you to limit this for each repository ingested. </p> <p>Please find an example below.</p> <p>For more information about options within the Class please follow the documentation under the code-reference section.</p>"},{"location":"getting-started/tokenisation-of-git-repo/#example","title":"Example","text":"<pre><code>from fleming.code_summary.repo_contents_tokenisation import GitHubRepoDataProcessor\nfrom pyspark.sql import SparkSession\n\n# Not required if using Databricks\nspark = SparkSession.builder.appName(\"RepoConcat\").getOrCreate()\n\norganization_name = 'company-x'\nrepo_list = ['repo1', 'repo2', 'repo3']\nnum_token_per_repo = 100000\npem_key = 'xxxxx'\npem_file = '/dbfs/FileStore/github_app/pem_key.pem'\n\ngithub_repo_data_processor = GitHubRepoDataProcessor(spark, organization_name, repo_list, num_token_per_repo, pem_key, pem_file)\nrepo_contents_df = github_repo_data_processor.data_collection()\nrepo_contents_df_concat = github_repo_data_processor.concatenate_repo_contents(repo_contents_df)\n</code></pre>"},{"location":"releases/fleming/","title":"Releases","text":""},{"location":"releases/fleming/#v004","title":"V0.0.4","text":""},{"location":"releases/fleming/#whats-changed","title":"What's Changed","text":"<ul> <li>Updating release notes by @BensGitShell in https://github.com/sede-open/Fleming/pull/21</li> <li>Adding code summary image by @BensGitShell in https://github.com/sede-open/Fleming/pull/22</li> <li>Update test_fourO_mini_summary.py by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/23</li> <li>Merge to ensure develop and main match by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/24</li> <li>Feature/00005 - Automation of pypi packaging upon releases. by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/25</li> <li>Develop by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/26</li> <li>Update ci.yml by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/27</li> <li>Feature/00006 to develop by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/28</li> <li>Develop to main by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/29</li> <li>Bump @babel/runtime from 7.26.9 to 7.27.0 in /src/fleming_ui by @dependabot in https://github.com/sede-open/Fleming/pull/30</li> <li>Bump next from 15.2.1 to 15.2.4 in /src/fleming_ui by @dependabot in https://github.com/sede-open/Fleming/pull/31</li> </ul>"},{"location":"releases/fleming/#new-contributors","title":"New Contributors","text":"<ul> <li>@dependabot made their first contribution in https://github.com/sede-open/Fleming/pull/30</li> </ul> <p>Full Changelog: https://github.com/sede-open/Fleming/compare/v0.0.3...v0.0.4</p>"},{"location":"releases/fleming/#v003","title":"V0.0.3","text":""},{"location":"releases/fleming/#whats-changed_1","title":"What's Changed","text":"<ul> <li>Develop to Main - Typo by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/16</li> <li>Improve docs - README by @spier in https://github.com/sede-open/Fleming/pull/15</li> <li>Feature/00004 - Extracting GitHub Repo Contents and Concatenating for Summarisation by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/17</li> <li>Develop to Main by @BensGitShell in https://github.com/sede-open/Fleming/pull/20</li> </ul>"},{"location":"releases/fleming/#new-contributors_1","title":"New Contributors","text":"<ul> <li>@spier made their first contribution in https://github.com/sede-open/Fleming/pull/15</li> <li>@BensGitShell made their first contribution in https://github.com/sede-open/Fleming/pull/20</li> </ul> <p>Full Changelog: https://github.com/sede-open/Fleming/compare/v0.0.2...v0.0.3</p>"},{"location":"releases/fleming/#v002","title":"V0.0.2","text":""},{"location":"releases/fleming/#whats-changed_2","title":"What's Changed","text":"<ul> <li>Env Bug Fix with JDK by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/11</li> </ul> <p>Full Changelog: https://github.com/sede-open/Fleming/compare/v0.0.1...v0.0.2</p>"},{"location":"releases/fleming/#v001","title":"V0.0.1","text":""},{"location":"releases/fleming/#whats-changed_3","title":"What's Changed","text":"<ul> <li>Initial commit for src code and documentation. by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/1</li> <li>Update LICENSE.md by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/2</li> <li>Update CODE_OF_CONDUCT.md by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/3</li> <li>Update setup.py with Fleming Path by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/4</li> </ul>"},{"location":"releases/fleming/#new-contributors_2","title":"New Contributors","text":"<ul> <li>@Amber-Rigg made their first contribution in https://github.com/sede-open/Fleming/pull/1</li> </ul> <p>Full Changelog: https://github.com/sede-open/Fleming/commits/v0.01</p>"},{"location":"releases/fleming/#whats-changed_4","title":"What's Changed","text":"<ul> <li>Initial commit for src code and documentation. by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/1</li> <li>Update LICENSE.md by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/2</li> <li>Update CODE_OF_CONDUCT.md by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/3</li> <li>Update setup.py with Fleming Path by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/4</li> <li>Feature/00002 - Inclusion of Mkdcos by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/5</li> <li>Update CI for mkdocs by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/6</li> <li>Develop to Main  by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/7</li> <li>Develop to Main  by @Amber-Rigg in https://github.com/sede-open/Fleming/pull/8</li> <li>Update mkdocs.yml by @doctorcdf27 in https://github.com/sede-open/Fleming/pull/9</li> </ul>"},{"location":"releases/fleming/#new-contributors_3","title":"New Contributors","text":"<ul> <li>@Amber-Rigg made their first contribution in https://github.com/sede-open/Fleming/pull/1</li> <li>@doctorcdf27 made their first contribution in https://github.com/sede-open/Fleming/pull/9</li> </ul> <p>Full Changelog: https://github.com/sede-open/Fleming/commits/v0.0.1</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""}]}